{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** *Disclaimer* ****\n",
    "\n",
    "The process described in this notebook replicates the method outlined in the report, using a combination of random search and grid search for hyperparameter tuning. However, it is important to note the following:\n",
    "\n",
    "- Differences in Computational Resources: The original experiments were conducted on the DTU's High-Performance Computing (HPC) cluster with extensive computational resources. This allowed for large-scale experiments with a broader search space and more iterations.\n",
    "\n",
    "- Simplification for Demonstration: This notebook is designed to be executable by anyone on a local machine or modest computational resources. As such:\n",
    "\n",
    "  - The data size and number of iterations for random and grid searches have been significantly reduced.\n",
    "  - The results produced by this notebook will differ from the report because the scale of experimentation is smaller.\n",
    "- Process Integrity: Despite these differences, the overall methodology remains consistent. The workflow, including random search followed by grid search on the refined parameter space, matches the approach used in the report.\n",
    "\n",
    "By making these adjustments, this notebook ensures accessibility while maintaining the integrity of the described hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To make sure that the notebooks stays as readable as possible, we won't implement code that is already implemented in other files.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION: Make sure to have the data properly extracted!!!!**\n",
    "It should be:\n",
    "```\n",
    "P12data \n",
    "|___split1 \n",
    "|___split2\n",
    "|___split3\n",
    "|___split4\n",
    "|___split5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to keep 5% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: split_1\n",
      "Processing file: P12data\\split_1\\test_physionet2012_1.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_1\\test_physionet2012_1.npy\n",
      "Processing file: P12data\\split_1\\train_physionet2012_1.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_1\\train_physionet2012_1.npy\n",
      "Processing file: P12data\\split_1\\validation_physionet2012_1.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_1\\validation_physionet2012_1.npy\n",
      "Processing directory: split_2\n",
      "Processing file: P12data\\split_2\\test_physionet2012_2.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_2\\test_physionet2012_2.npy\n",
      "Processing file: P12data\\split_2\\train_physionet2012_2.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_2\\train_physionet2012_2.npy\n",
      "Processing file: P12data\\split_2\\validation_physionet2012_2.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_2\\validation_physionet2012_2.npy\n",
      "Processing directory: split_3\n",
      "Processing file: P12data\\split_3\\test_physionet2012_3.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_3\\test_physionet2012_3.npy\n",
      "Processing file: P12data\\split_3\\train_physionet2012_3.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_3\\train_physionet2012_3.npy\n",
      "Processing file: P12data\\split_3\\validation_physionet2012_3.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_3\\validation_physionet2012_3.npy\n",
      "Processing directory: split_4\n",
      "Processing file: P12data\\split_4\\test_physionet2012_4.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_4\\test_physionet2012_4.npy\n",
      "Processing file: P12data\\split_4\\train_physionet2012_4.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_4\\train_physionet2012_4.npy\n",
      "Processing file: P12data\\split_4\\validation_physionet2012_4.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_4\\validation_physionet2012_4.npy\n",
      "Processing directory: split_5\n",
      "Processing file: P12data\\split_5\\test_physionet2012_5.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_5\\test_physionet2012_5.npy\n",
      "Processing file: P12data\\split_5\\train_physionet2012_5.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_5\\train_physionet2012_5.npy\n",
      "Processing file: P12data\\split_5\\validation_physionet2012_5.npy\n",
      "Replaced file with 5% sampled data: P12data\\split_5\\validation_physionet2012_5.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def process_splits(base_dir, file_prefixes=[\"test_physionet2012\", \"train_physionet2012\", \"validation_physionet2012\"]):\n",
    "    \"\"\"\n",
    "    Iterates over directories named split1, split2, ..., processes the three files inside\n",
    "    to keep only 5% of the rows, and replaces the original files.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): The base directory containing split directories.\n",
    "        file_prefixes (list): List of file prefixes to process within each split directory.\n",
    "    \"\"\"\n",
    "    # Iterate over directories in the base directory\n",
    "    for split_dir in os.listdir(base_dir):\n",
    "        split_path = os.path.join(base_dir, split_dir)\n",
    "        if os.path.isdir(split_path) and split_dir.startswith(\"split\"):\n",
    "            print(f\"Processing directory: {split_dir}\")\n",
    "\n",
    "            # Process each file in the split directory\n",
    "            for prefix in file_prefixes:\n",
    "                file_name = f\"{prefix}_{split_dir[-1]}.npy\"\n",
    "                file_path = os.path.join(split_path, file_name)\n",
    "\n",
    "                if os.path.exists(file_path):\n",
    "                    print(f\"Processing file: {file_path}\")\n",
    "                    try:\n",
    "                        # Load data from .npy file\n",
    "                        data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "                        # Randomly select 5% of rows\n",
    "                        sampled_data = data[np.random.choice(len(data), int(0.05 * len(data)), replace=False)]\n",
    "\n",
    "                        # Save the sampled data back to the same file\n",
    "                        np.save(file_path, sampled_data)\n",
    "                        print(f\"Replaced file with 5% sampled data: {file_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file_path}: {e}\")\n",
    "                else:\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Example usage\n",
    "base_directory = \"P12data\"  # Replace with the path to the directory containing the split folders\n",
    "process_splits(base_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Trying combination 1/50:\n",
      "hidden_size=32\n",
      "num_layers=1\n",
      "dropout_rate=0.1\n",
      "learning_rate=0.001\n",
      "batch_size=16\n",
      "class_weights=[1.0, 7.143]\n",
      "bidirectional=False\n",
      "\n",
      "Split 1, Epoch 1/100\n",
      "Train Loss: 0.6941, Val Loss: 0.6864\n",
      "Val AUROC: 0.5515, Val AUPRC: 0.1843\n",
      "\n",
      "Split 1, Epoch 2/100\n",
      "Train Loss: 0.6798, Val Loss: 0.6871\n",
      "Val AUROC: 0.5833, Val AUPRC: 0.2027\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 3/100\n",
      "Train Loss: 0.6599, Val Loss: 0.6674\n",
      "Val AUROC: 0.6201, Val AUPRC: 0.2193\n",
      "\n",
      "Split 1, Epoch 4/100\n",
      "Train Loss: 0.5917, Val Loss: 0.6803\n",
      "Val AUROC: 0.6716, Val AUPRC: 0.4071\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 5/100\n",
      "Train Loss: 0.5510, Val Loss: 0.6914\n",
      "Val AUROC: 0.6863, Val AUPRC: 0.3015\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 6/100\n",
      "Train Loss: 0.5513, Val Loss: 0.8201\n",
      "Val AUROC: 0.6667, Val AUPRC: 0.2770\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 7/100\n",
      "Train Loss: 0.5444, Val Loss: 0.9712\n",
      "Val AUROC: 0.6569, Val AUPRC: 0.2625\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 8/100\n",
      "Train Loss: 0.4730, Val Loss: 0.7370\n",
      "Val AUROC: 0.6838, Val AUPRC: 0.2726\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 9/100\n",
      "Train Loss: 0.4396, Val Loss: 0.7137\n",
      "Val AUROC: 0.7230, Val AUPRC: 0.3157\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 10/100\n",
      "Train Loss: 0.4167, Val Loss: 0.7404\n",
      "Val AUROC: 0.7328, Val AUPRC: 0.3628\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 11/100\n",
      "Train Loss: 0.4378, Val Loss: 0.9108\n",
      "Val AUROC: 0.7304, Val AUPRC: 0.3321\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1, Epoch 12/100\n",
      "Train Loss: 0.4116, Val Loss: 0.7358\n",
      "Val AUROC: 0.7451, Val AUPRC: 0.3408\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Split 1, Epoch 13/100\n",
      "Train Loss: 0.4373, Val Loss: 0.8107\n",
      "Val AUROC: 0.7304, Val AUPRC: 0.3538\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered at epoch 12\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mean_auroc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdssm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DSSM\n\u001b[0;32m      7\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Model parameters\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m],  \u001b[38;5;66;03m# Capacity of the model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m],  \u001b[38;5;66;03m# Whether to use a bidirectional LSTM  \u001b[39;00m\n\u001b[0;32m     21\u001b[0m }\n\u001b[1;32m---> 23\u001b[0m best_params_random_search, best_metrics_random_search, all_results_random_search \u001b[38;5;241m=\u001b[39m random_search(n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, param_grid \u001b[38;5;241m=\u001b[39m param_grid, results_path_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/dssm_output/random_search_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\scripts\\random_search.py:143\u001b[0m, in \u001b[0;36mrandom_search\u001b[1;34m(n_iter, param_grid, results_path_str)\u001b[0m\n\u001b[0;32m    121\u001b[0m base_model_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m37\u001b[39m,\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic_input_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    127\u001b[0m }\n\u001b[0;32m    129\u001b[0m base_training_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weights\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1.163\u001b[39m, \u001b[38;5;241m7.143\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m     }\n\u001b[0;32m    141\u001b[0m }\n\u001b[1;32m--> 143\u001b[0m best_params, best_metrics, all_results \u001b[38;5;241m=\u001b[39m train_with_randomized_search_cv(\n\u001b[0;32m    144\u001b[0m     DSSM,\n\u001b[0;32m    145\u001b[0m     base_model_params,\n\u001b[0;32m    146\u001b[0m     base_training_params,\n\u001b[0;32m    147\u001b[0m     device,\n\u001b[0;32m    148\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39mn_iter,\n\u001b[0;32m    149\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid\n\u001b[0;32m    150\u001b[0m )\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results_path_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     results_path_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model_outputs/dssm_output/random_search_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\scripts\\random_search.py:84\u001b[0m, in \u001b[0;36mtrain_with_randomized_search_cv\u001b[1;34m(model_class, base_model_params, base_training_params, device, n_iter, param_grid)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m metrics, _ \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m     81\u001b[0m     model_class, model_params, training_params, device, split_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n\u001b[1;32m---> 84\u001b[0m combined_score \u001b[38;5;241m=\u001b[39m calculate_combined_score(metrics)\n\u001b[0;32m     85\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m combined_score\n\u001b[0;32m     87\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: sampled_params,\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics\u001b[39m\u001b[38;5;124m'\u001b[39m: metrics\n\u001b[0;32m     90\u001b[0m })\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\train.py:340\u001b[0m, in \u001b[0;36mcalculate_combined_score\u001b[1;34m(metrics)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate a combined score giving higher weight to AUPRC due to class imbalance\"\"\"\u001b[39;00m\n\u001b[0;32m    333\u001b[0m weights \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_auroc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_auprc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.4\u001b[39m,  \u001b[38;5;66;03m# Higher weight due to class imbalance\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m    337\u001b[0m }\n\u001b[0;32m    339\u001b[0m combined_score \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 340\u001b[0m         weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_auroc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_auroc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    341\u001b[0m         weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_auprc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_auprc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    342\u001b[0m         weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    343\u001b[0m )\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_score\n",
      "\u001b[1;31mKeyError\u001b[0m: 'mean_auroc'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import torch\n",
    "from scripts.random_search import random_search\n",
    "from models.dssm import DSSM\n",
    "\n",
    "param_grid = {\n",
    "    # Model parameters\n",
    "    'hidden_size': [32, 64, 128, 256],  # Capacity of the model\n",
    "    'num_layers': [1, 2, 3],  # Depth of LSTM\n",
    "    'dropout_rate': [0.1, 0.2, 0.3, 0.4],   # Regularization strength\n",
    "\n",
    "    # Training parameters\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],  # Regularization strength\n",
    "    'batch_size': [16, 32, 64],  # Batch sizes\n",
    "    'class_weights': [\n",
    "         [1.0, 7.143], [1.0, 8.5],\n",
    "        [1.0, 6.0], [1.0, 5.0] \n",
    "    ],\n",
    "    'bidirectional': [True, False],  # Whether to use a bidirectional LSTM  \n",
    "}\n",
    "\n",
    "best_params_random_search, best_metrics_random_search, all_results_random_search = random_search(n_iter = 50, param_grid = param_grid, results_path_str='model_outputs/dssm_output/random_search_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>class_weights</th>\n",
       "      <th>bidirectional</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auprc</th>\n",
       "      <th>auroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.273797</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.948148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.309857</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.654242</td>\n",
       "      <td>0.948148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.279902</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.565714</td>\n",
       "      <td>0.940741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.320171</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.656923</td>\n",
       "      <td>0.937037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.368028</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.613651</td>\n",
       "      <td>0.937037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.345470</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.937037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.305940</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.550909</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.361920</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.551526</td>\n",
       "      <td>0.929630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.436509</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.929630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.267838</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.532179</td>\n",
       "      <td>0.929630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.390516</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.413492</td>\n",
       "      <td>0.929630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.313531</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.536061</td>\n",
       "      <td>0.925926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.360734</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.468651</td>\n",
       "      <td>0.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.328608</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.524156</td>\n",
       "      <td>0.918519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.470911</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.517143</td>\n",
       "      <td>0.918519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.404934</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.918519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.413250</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.511061</td>\n",
       "      <td>0.918519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.382369</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.604156</td>\n",
       "      <td>0.914815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.366441</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.914815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.377767</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.517103</td>\n",
       "      <td>0.914815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.364175</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.603205</td>\n",
       "      <td>0.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.387869</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.362083</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.494881</td>\n",
       "      <td>0.907407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.388536</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.903704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.425072</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.404038</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.423381</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.536142</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 5.0)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.356908</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.344156</td>\n",
       "      <td>0.896296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 5.0)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.383417</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.372727</td>\n",
       "      <td>0.896296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.406237</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.323203</td>\n",
       "      <td>0.892593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.429590</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.507372</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.510334</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.492121</td>\n",
       "      <td>0.885185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.517406</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.329298</td>\n",
       "      <td>0.881481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 5.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.572012</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.355621</td>\n",
       "      <td>0.870370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.554351</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.855556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.513374</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.497917</td>\n",
       "      <td>0.851852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.674827</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.309158</td>\n",
       "      <td>0.848148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.740908</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.346340</td>\n",
       "      <td>0.840741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.701949</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.303795</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.652954</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.431471</td>\n",
       "      <td>0.814815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.725638</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.248412</td>\n",
       "      <td>0.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.691980</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.200974</td>\n",
       "      <td>0.768519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 6.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.491131</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.768519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.545201</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.396032</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.642129</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.202757</td>\n",
       "      <td>0.712963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>32</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.587582</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.125025</td>\n",
       "      <td>0.644444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.857257</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.124624</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>16</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.643490</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.607407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 8.5)</td>\n",
       "      <td>True</td>\n",
       "      <td>0.784727</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.128829</td>\n",
       "      <td>0.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 7.143)</td>\n",
       "      <td>False</td>\n",
       "      <td>0.881673</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.122750</td>\n",
       "      <td>0.574074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>64</td>\n",
       "      <td>(1.0, 5.0)</td>\n",
       "      <td>False</td>\n",
       "      <td>1.357776</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.139563</td>\n",
       "      <td>0.537037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size  num_layers  dropout_rate  learning_rate  batch_size  \\\n",
       "44           32           2           0.2         0.0010          64   \n",
       "36           64           3           0.4         0.0010          64   \n",
       "25          256           1           0.2         0.0001          16   \n",
       "22          128           1           0.4         0.0001          16   \n",
       "41          128           1           0.1         0.0001          64   \n",
       "26          128           1           0.2         0.0001          64   \n",
       "0           256           3           0.1         0.0001          16   \n",
       "43           32           3           0.2         0.0001          32   \n",
       "30          256           2           0.2         0.0100          16   \n",
       "9            32           2           0.1         0.0010          16   \n",
       "10          128           2           0.1         0.0010          16   \n",
       "17           32           3           0.3         0.0010          32   \n",
       "6            32           3           0.1         0.0010          16   \n",
       "7           256           1           0.1         0.0001          32   \n",
       "19           64           1           0.2         0.0010          16   \n",
       "33           32           1           0.1         0.0001          32   \n",
       "1           256           3           0.3         0.0001          16   \n",
       "42           32           3           0.4         0.0001          32   \n",
       "47          128           3           0.3         0.0010          64   \n",
       "23          128           2           0.3         0.0001          16   \n",
       "12          256           3           0.4         0.0010          32   \n",
       "13          128           3           0.1         0.0010          32   \n",
       "21           32           1           0.3         0.0010          16   \n",
       "15           32           2           0.3         0.0010          32   \n",
       "45          128           3           0.4         0.0001          16   \n",
       "20           32           2           0.1         0.0010          32   \n",
       "28          256           2           0.1         0.0010          64   \n",
       "11          128           2           0.2         0.0010          16   \n",
       "38           32           1           0.2         0.0100          64   \n",
       "3            32           2           0.4         0.0010          16   \n",
       "14          256           3           0.2         0.0010          32   \n",
       "24           32           2           0.1         0.0100          32   \n",
       "29           32           1           0.3         0.0100          32   \n",
       "46           32           1           0.1         0.0001          64   \n",
       "5            32           2           0.4         0.0001          64   \n",
       "2           256           1           0.3         0.0010          16   \n",
       "31           64           1           0.1         0.0100          64   \n",
       "39           32           2           0.2         0.0100          64   \n",
       "37          128           2           0.1         0.0100          64   \n",
       "8           256           2           0.3         0.0100          16   \n",
       "32          256           1           0.4         0.0100          32   \n",
       "34          256           1           0.4         0.0100          16   \n",
       "35           64           1           0.4         0.0001          16   \n",
       "18          128           3           0.2         0.0100          16   \n",
       "49          128           1           0.2         0.0100          32   \n",
       "16          128           3           0.1         0.0100          64   \n",
       "27          128           3           0.3         0.0100          16   \n",
       "40          128           2           0.4         0.0100          64   \n",
       "48          128           3           0.1         0.0100          64   \n",
       "4           128           2           0.1         0.0100          64   \n",
       "\n",
       "   class_weights  bidirectional      loss  accuracy     auprc     auroc  \n",
       "44    (1.0, 8.5)           True  0.273797  0.881356  0.683333  0.948148  \n",
       "36  (1.0, 7.143)          False  0.309857  0.864407  0.654242  0.948148  \n",
       "25    (1.0, 6.0)           True  0.279902  0.898305  0.565714  0.940741  \n",
       "22  (1.0, 7.143)           True  0.320171  0.847458  0.656923  0.937037  \n",
       "41    (1.0, 8.5)           True  0.368028  0.779661  0.613651  0.937037  \n",
       "26  (1.0, 7.143)           True  0.345470  0.830508  0.576923  0.937037  \n",
       "0     (1.0, 6.0)           True  0.305940  0.898305  0.550909  0.933333  \n",
       "43  (1.0, 7.143)          False  0.361920  0.847458  0.551526  0.929630  \n",
       "30    (1.0, 8.5)           True  0.436509  0.677966  0.650000  0.929630  \n",
       "9     (1.0, 6.0)           True  0.267838  0.898305  0.532179  0.929630  \n",
       "10    (1.0, 8.5)           True  0.390516  0.796610  0.413492  0.929630  \n",
       "17    (1.0, 8.5)           True  0.313531  0.830508  0.536061  0.925926  \n",
       "6     (1.0, 6.0)           True  0.360734  0.847458  0.468651  0.922222  \n",
       "7   (1.0, 7.143)          False  0.328608  0.847458  0.524156  0.918519  \n",
       "19  (1.0, 7.143)           True  0.470911  0.864407  0.517143  0.918519  \n",
       "33  (1.0, 7.143)           True  0.404934  0.779661  0.577778  0.918519  \n",
       "1     (1.0, 8.5)          False  0.413250  0.881356  0.511061  0.918519  \n",
       "42  (1.0, 7.143)           True  0.382369  0.830508  0.604156  0.914815  \n",
       "47  (1.0, 7.143)           True  0.366441  0.830508  0.453333  0.914815  \n",
       "23  (1.0, 7.143)          False  0.377767  0.864407  0.517103  0.914815  \n",
       "12    (1.0, 6.0)          False  0.364175  0.813559  0.603205  0.911111  \n",
       "13  (1.0, 7.143)          False  0.387869  0.830508  0.438095  0.907407  \n",
       "21  (1.0, 7.143)          False  0.362083  0.847458  0.494881  0.907407  \n",
       "15    (1.0, 8.5)           True  0.388536  0.830508  0.480000  0.903704  \n",
       "45  (1.0, 7.143)          False  0.425072  0.813559  0.404038  0.900000  \n",
       "20  (1.0, 7.143)          False  0.423381  0.762712  0.536142  0.900000  \n",
       "28    (1.0, 5.0)           True  0.356908  0.830508  0.344156  0.896296  \n",
       "11    (1.0, 5.0)           True  0.383417  0.847458  0.372727  0.896296  \n",
       "38    (1.0, 6.0)          False  0.406237  0.847458  0.323203  0.892593  \n",
       "3   (1.0, 7.143)          False  0.429590  0.813559  0.507372  0.888889  \n",
       "14    (1.0, 6.0)          False  0.510334  0.779661  0.492121  0.885185  \n",
       "24    (1.0, 8.5)           True  0.517406  0.881356  0.329298  0.881481  \n",
       "29    (1.0, 5.0)          False  0.572012  0.881356  0.355621  0.870370  \n",
       "46  (1.0, 7.143)          False  0.554351  0.694915  0.433333  0.855556  \n",
       "5     (1.0, 8.5)           True  0.513374  0.711864  0.497917  0.851852  \n",
       "2     (1.0, 6.0)          False  0.674827  0.847458  0.309158  0.848148  \n",
       "31    (1.0, 8.5)           True  0.740908  0.796610  0.346340  0.840741  \n",
       "39  (1.0, 7.143)          False  0.701949  0.745763  0.303795  0.833333  \n",
       "37    (1.0, 6.0)          False  0.652954  0.881356  0.431471  0.814815  \n",
       "8     (1.0, 6.0)           True  0.725638  0.915254  0.248412  0.796296  \n",
       "32  (1.0, 7.143)          False  0.691980  0.237288  0.200974  0.768519  \n",
       "34    (1.0, 6.0)          False  0.491131  0.915254  0.166667  0.768519  \n",
       "35  (1.0, 7.143)          False  0.545201  0.728814  0.396032  0.766667  \n",
       "18  (1.0, 7.143)          False  0.642129  0.338983  0.202757  0.712963  \n",
       "49  (1.0, 7.143)           True  0.587582  0.474576  0.125025  0.644444  \n",
       "16  (1.0, 7.143)          False  0.857257  0.406780  0.124624  0.611111  \n",
       "27  (1.0, 7.143)          False  0.643490  0.423729  0.120370  0.607407  \n",
       "40    (1.0, 8.5)           True  0.784727  0.372881  0.128829  0.592593  \n",
       "48  (1.0, 7.143)          False  0.881673  0.355932  0.122750  0.574074  \n",
       "4     (1.0, 5.0)          False  1.357776  0.423729  0.139563  0.537037  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def show_results(data):\n",
    "    # Transform the data into a pandas DataFrame\n",
    "    rows = []\n",
    "    for entry in data:\n",
    "        combined = {**entry[\"params\"], **entry[\"metrics\"]}\n",
    "        # Convert class_weights to a tuple to make it hashable\n",
    "        combined[\"class_weights\"] = tuple(combined[\"class_weights\"])\n",
    "        rows.append(combined)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    return df\n",
    "\n",
    "df = show_results(all_results_random_search)\n",
    "df.sort_values(\"combined_score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we keep the hyperparameter values from the top 5 best hyperparameter combinations and try every combination between them. In this case, we make sure that not more than 2 values for each hyperparameter is added (prioritizing the ones with the best metrics), so the grid search space doesn't explode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due to high runtimes, we're going to slash the number of possible combinations by half by eliminating the option `bidirectional: false` because it's almost not present in the best combinations, as we can see in the table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_size': [32, 64],\n",
       " 'num_layers': [2, 3],\n",
       " 'dropout_rate': [0.2, 0.4],\n",
       " 'learning_rate': [0.0001, 0.001],\n",
       " 'batch_size': [64, 16],\n",
       " 'class_weights': [(1.0, 7.143), (1.0, 8.5)],\n",
       " 'bidirectional': [False, True]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Sort by AUROC to get the top 5 results\n",
    "sorted_data = sorted(all_results_random_search, key=lambda x: x['metrics']['combined_score'], reverse=True)\n",
    "top_5 = sorted_data[:5]\n",
    "# Step 2: Collect unique values for each hyperparameter in the top 5 results\n",
    "unique_values = {\n",
    "    \"hidden_size\": set(),\n",
    "    \"num_layers\": set(),\n",
    "    \"dropout_rate\": set(),\n",
    "    \"learning_rate\": set(),\n",
    "    \"batch_size\": set(),\n",
    "    \"class_weights\": set(),\n",
    "    \"bidirectional\": set()\n",
    "}\n",
    "for entry in top_5:\n",
    "    params = entry[\"params\"]\n",
    "    for key in unique_values.keys():\n",
    "        if len(unique_values[key]) < 2: \n",
    "            unique_values[key].add(tuple(params[key]) if isinstance(params[key], list) else params[key])\n",
    "\n",
    "# Convert sets to lists for better readability\n",
    "unique_values = {key: list(values) for key, values in unique_values.items()}\n",
    "\n",
    "# Bidirectional only true\n",
    "unique_values[\"bidirectional\"] = [True]\n",
    "\n",
    "# Display the result\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total parameter combinations to try: 128\n",
      "\n",
      "Trying combination 1/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=64, class_weights=(1.0, 7.143), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6988, Val Loss: 0.7008\n",
      "Val AUROC: 0.6275, Val AUPRC: 0.2208\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6975, Val Loss: 0.7000\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.2089\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6920, Val Loss: 0.6995\n",
      "Val AUROC: 0.6054, Val AUPRC: 0.2086\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6951, Val Loss: 0.6988\n",
      "Val AUROC: 0.6103, Val AUPRC: 0.2363\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6962, Val Loss: 0.6982\n",
      "Val AUROC: 0.6201, Val AUPRC: 0.2442\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6938, Val Loss: 0.6976\n",
      "Val AUROC: 0.6127, Val AUPRC: 0.2415\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6952, Val Loss: 0.6971\n",
      "Val AUROC: 0.5931, Val AUPRC: 0.2340\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6912, Val Loss: 0.6965\n",
      "Val AUROC: 0.5858, Val AUPRC: 0.2927\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6933, Val Loss: 0.6959\n",
      "Val AUROC: 0.5907, Val AUPRC: 0.2959\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6909, Val Loss: 0.6953\n",
      "Val AUROC: 0.5858, Val AUPRC: 0.2942\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6897, Val Loss: 0.6947\n",
      "Val AUROC: 0.5882, Val AUPRC: 0.3011\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6890, Val Loss: 0.6942\n",
      "Val AUROC: 0.5882, Val AUPRC: 0.3011\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6871, Val Loss: 0.6936\n",
      "Val AUROC: 0.5882, Val AUPRC: 0.3011\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6888, Val Loss: 0.6934\n",
      "Val AUROC: 0.5907, Val AUPRC: 0.3017\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6871, Val Loss: 0.6930\n",
      "Val AUROC: 0.5907, Val AUPRC: 0.3015\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.6870, Val Loss: 0.6927\n",
      "Val AUROC: 0.5907, Val AUPRC: 0.3014\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.6855, Val Loss: 0.6923\n",
      "Val AUROC: 0.5833, Val AUPRC: 0.2993\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.6854, Val Loss: 0.6920\n",
      "Val AUROC: 0.5833, Val AUPRC: 0.2993\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.6857, Val Loss: 0.6915\n",
      "Val AUROC: 0.5833, Val AUPRC: 0.2993\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.6815, Val Loss: 0.6911\n",
      "Val AUROC: 0.5809, Val AUPRC: 0.2986\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.6824\n",
      "accuracy: 0.7288\n",
      "auprc: 0.1732\n",
      "auroc: 0.6259\n",
      "\n",
      "New best parameters found!\n",
      "New best combined score: 0.4757\n",
      "AUROC: 0.6259\n",
      "AUPRC: 0.1732\n",
      "Accuracy: 0.7288\n",
      "\n",
      "Trying combination 2/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=64, class_weights=(1.0, 7.143), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6968, Val Loss: 0.6941\n",
      "Val AUROC: 0.4608, Val AUPRC: 0.1379\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6990, Val Loss: 0.6936\n",
      "Val AUROC: 0.4608, Val AUPRC: 0.1403\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6955, Val Loss: 0.6933\n",
      "Val AUROC: 0.4657, Val AUPRC: 0.1398\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6954, Val Loss: 0.6930\n",
      "Val AUROC: 0.4657, Val AUPRC: 0.1423\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6920, Val Loss: 0.6927\n",
      "Val AUROC: 0.4779, Val AUPRC: 0.1615\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6939, Val Loss: 0.6925\n",
      "Val AUROC: 0.4706, Val AUPRC: 0.1605\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6911, Val Loss: 0.6922\n",
      "Val AUROC: 0.4706, Val AUPRC: 0.1611\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6900, Val Loss: 0.6919\n",
      "Val AUROC: 0.4755, Val AUPRC: 0.1660\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6930, Val Loss: 0.6916\n",
      "Val AUROC: 0.4877, Val AUPRC: 0.1749\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6916, Val Loss: 0.6914\n",
      "Val AUROC: 0.4975, Val AUPRC: 0.1846\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6892, Val Loss: 0.6912\n",
      "Val AUROC: 0.5000, Val AUPRC: 0.2135\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6863, Val Loss: 0.6909\n",
      "Val AUROC: 0.5074, Val AUPRC: 0.2959\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6871, Val Loss: 0.6906\n",
      "Val AUROC: 0.5221, Val AUPRC: 0.3197\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6887, Val Loss: 0.6901\n",
      "Val AUROC: 0.5466, Val AUPRC: 0.3273\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6855, Val Loss: 0.6897\n",
      "Val AUROC: 0.5466, Val AUPRC: 0.3277\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.6844, Val Loss: 0.6891\n",
      "Val AUROC: 0.5564, Val AUPRC: 0.3324\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.6862, Val Loss: 0.6885\n",
      "Val AUROC: 0.5760, Val AUPRC: 0.3378\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.6818, Val Loss: 0.6878\n",
      "Val AUROC: 0.5882, Val AUPRC: 0.3406\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.6814, Val Loss: 0.6869\n",
      "Val AUROC: 0.6005, Val AUPRC: 0.3475\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.6810, Val Loss: 0.6859\n",
      "Val AUROC: 0.6005, Val AUPRC: 0.3862\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.6836\n",
      "accuracy: 0.4068\n",
      "auprc: 0.3707\n",
      "auroc: 0.7963\n",
      "\n",
      "New best parameters found!\n",
      "New best combined score: 0.5092\n",
      "AUROC: 0.7963\n",
      "AUPRC: 0.3707\n",
      "Accuracy: 0.4068\n",
      "\n",
      "Trying combination 3/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=64, class_weights=(1.0, 8.5), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6936, Val Loss: 0.6827\n",
      "Val AUROC: 0.6176, Val AUPRC: 0.2028\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6915, Val Loss: 0.6823\n",
      "Val AUROC: 0.6299, Val AUPRC: 0.2132\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6934, Val Loss: 0.6819\n",
      "Val AUROC: 0.6495, Val AUPRC: 0.2258\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6898, Val Loss: 0.6816\n",
      "Val AUROC: 0.6667, Val AUPRC: 0.2744\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6913, Val Loss: 0.6811\n",
      "Val AUROC: 0.6765, Val AUPRC: 0.3772\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6885, Val Loss: 0.6808\n",
      "Val AUROC: 0.6740, Val AUPRC: 0.3791\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6852, Val Loss: 0.6804\n",
      "Val AUROC: 0.6765, Val AUPRC: 0.3896\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6889, Val Loss: 0.6799\n",
      "Val AUROC: 0.6765, Val AUPRC: 0.3909\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6871, Val Loss: 0.6797\n",
      "Val AUROC: 0.6765, Val AUPRC: 0.3954\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6860, Val Loss: 0.6793\n",
      "Val AUROC: 0.6789, Val AUPRC: 0.4174\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6882, Val Loss: 0.6790\n",
      "Val AUROC: 0.6838, Val AUPRC: 0.4208\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6836, Val Loss: 0.6787\n",
      "Val AUROC: 0.6789, Val AUPRC: 0.4036\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6839, Val Loss: 0.6784\n",
      "Val AUROC: 0.6691, Val AUPRC: 0.3968\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6866, Val Loss: 0.6781\n",
      "Val AUROC: 0.6765, Val AUPRC: 0.4073\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6814, Val Loss: 0.6777\n",
      "Val AUROC: 0.6838, Val AUPRC: 0.4357\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.6786, Val Loss: 0.6772\n",
      "Val AUROC: 0.6691, Val AUPRC: 0.4123\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.6833, Val Loss: 0.6768\n",
      "Val AUROC: 0.6716, Val AUPRC: 0.4729\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.6793, Val Loss: 0.6763\n",
      "Val AUROC: 0.6740, Val AUPRC: 0.4748\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.6826, Val Loss: 0.6760\n",
      "Val AUROC: 0.6642, Val AUPRC: 0.4472\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.6813, Val Loss: 0.6756\n",
      "Val AUROC: 0.6593, Val AUPRC: 0.4247\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.7027\n",
      "accuracy: 0.1186\n",
      "auprc: 0.2328\n",
      "auroc: 0.6222\n",
      "\n",
      "Trying combination 4/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=64, class_weights=(1.0, 8.5), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6995, Val Loss: 0.7011\n",
      "Val AUROC: 0.4461, Val AUPRC: 0.2562\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6999, Val Loss: 0.6992\n",
      "Val AUROC: 0.4632, Val AUPRC: 0.2659\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6966, Val Loss: 0.6979\n",
      "Val AUROC: 0.4804, Val AUPRC: 0.1765\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6945, Val Loss: 0.6966\n",
      "Val AUROC: 0.4608, Val AUPRC: 0.1547\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6935, Val Loss: 0.6951\n",
      "Val AUROC: 0.4755, Val AUPRC: 0.1492\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6914, Val Loss: 0.6935\n",
      "Val AUROC: 0.5000, Val AUPRC: 0.1523\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6930, Val Loss: 0.6919\n",
      "Val AUROC: 0.5441, Val AUPRC: 0.1692\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6909, Val Loss: 0.6904\n",
      "Val AUROC: 0.5515, Val AUPRC: 0.1728\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6890, Val Loss: 0.6892\n",
      "Val AUROC: 0.5637, Val AUPRC: 0.2087\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6899, Val Loss: 0.6875\n",
      "Val AUROC: 0.5784, Val AUPRC: 0.2160\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6896, Val Loss: 0.6861\n",
      "Val AUROC: 0.5809, Val AUPRC: 0.1988\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6880, Val Loss: 0.6852\n",
      "Val AUROC: 0.5882, Val AUPRC: 0.2040\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6860, Val Loss: 0.6846\n",
      "Val AUROC: 0.5956, Val AUPRC: 0.2132\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6871, Val Loss: 0.6842\n",
      "Val AUROC: 0.6005, Val AUPRC: 0.2103\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6879, Val Loss: 0.6836\n",
      "Val AUROC: 0.6029, Val AUPRC: 0.2209\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.6821, Val Loss: 0.6834\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.2458\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.6819, Val Loss: 0.6821\n",
      "Val AUROC: 0.6250, Val AUPRC: 0.3112\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.6837, Val Loss: 0.6807\n",
      "Val AUROC: 0.6324, Val AUPRC: 0.3107\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.6792, Val Loss: 0.6794\n",
      "Val AUROC: 0.6446, Val AUPRC: 0.3195\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.6762, Val Loss: 0.6785\n",
      "Val AUROC: 0.6495, Val AUPRC: 0.3267\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.6975\n",
      "accuracy: 0.1017\n",
      "auprc: 0.2956\n",
      "auroc: 0.5778\n",
      "\n",
      "Trying combination 5/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=16, class_weights=(1.0, 7.143), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6914, Val Loss: 0.6884\n",
      "Val AUROC: 0.6005, Val AUPRC: 0.2448\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6907, Val Loss: 0.6851\n",
      "Val AUROC: 0.6127, Val AUPRC: 0.2475\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6922, Val Loss: 0.6902\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.2414\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6917, Val Loss: 0.6865\n",
      "Val AUROC: 0.6324, Val AUPRC: 0.2534\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6873, Val Loss: 0.6875\n",
      "Val AUROC: 0.6348, Val AUPRC: 0.2355\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6840, Val Loss: 0.6833\n",
      "Val AUROC: 0.6176, Val AUPRC: 0.2186\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6905, Val Loss: 0.6789\n",
      "Val AUROC: 0.6250, Val AUPRC: 0.2268\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6905, Val Loss: 0.6783\n",
      "Val AUROC: 0.6324, Val AUPRC: 0.2289\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6867, Val Loss: 0.6879\n",
      "Val AUROC: 0.6250, Val AUPRC: 0.2214\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6842, Val Loss: 0.6910\n",
      "Val AUROC: 0.6250, Val AUPRC: 0.2235\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6850, Val Loss: 0.6875\n",
      "Val AUROC: 0.6299, Val AUPRC: 0.2245\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6879, Val Loss: 0.6903\n",
      "Val AUROC: 0.6373, Val AUPRC: 0.2274\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6854, Val Loss: 0.6882\n",
      "Val AUROC: 0.6373, Val AUPRC: 0.2240\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6851, Val Loss: 0.6900\n",
      "Val AUROC: 0.6348, Val AUPRC: 0.2242\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6814, Val Loss: 0.6854\n",
      "Val AUROC: 0.6299, Val AUPRC: 0.2191\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.6807, Val Loss: 0.6869\n",
      "Val AUROC: 0.6422, Val AUPRC: 0.2296\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.6769, Val Loss: 0.6881\n",
      "Val AUROC: 0.6373, Val AUPRC: 0.2297\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.6728, Val Loss: 0.6828\n",
      "Val AUROC: 0.6422, Val AUPRC: 0.2316\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered at epoch 17\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.6680\n",
      "accuracy: 0.7627\n",
      "auprc: 0.1721\n",
      "auroc: 0.6815\n",
      "\n",
      "Trying combination 6/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=16, class_weights=(1.0, 7.143), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6917, Val Loss: 0.6938\n",
      "Val AUROC: 0.5931, Val AUPRC: 0.2244\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6903, Val Loss: 0.6922\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.2495\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6885, Val Loss: 0.6936\n",
      "Val AUROC: 0.6275, Val AUPRC: 0.3481\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6883, Val Loss: 0.6916\n",
      "Val AUROC: 0.6275, Val AUPRC: 0.3666\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6827, Val Loss: 0.6966\n",
      "Val AUROC: 0.6176, Val AUPRC: 0.4012\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6825, Val Loss: 0.6856\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.3575\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6836, Val Loss: 0.6953\n",
      "Val AUROC: 0.6299, Val AUPRC: 0.3654\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6807, Val Loss: 0.6873\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.3941\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6796, Val Loss: 0.6876\n",
      "Val AUROC: 0.6250, Val AUPRC: 0.3966\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6754, Val Loss: 0.6853\n",
      "Val AUROC: 0.6250, Val AUPRC: 0.3957\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6717, Val Loss: 0.6779\n",
      "Val AUROC: 0.6225, Val AUPRC: 0.3942\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6670, Val Loss: 0.6642\n",
      "Val AUROC: 0.6054, Val AUPRC: 0.3851\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6558, Val Loss: 0.6556\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.3841\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6317, Val Loss: 0.6568\n",
      "Val AUROC: 0.6348, Val AUPRC: 0.3926\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6102, Val Loss: 0.6422\n",
      "Val AUROC: 0.6397, Val AUPRC: 0.3881\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.5920, Val Loss: 0.6330\n",
      "Val AUROC: 0.6765, Val AUPRC: 0.4000\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.5463, Val Loss: 0.6401\n",
      "Val AUROC: 0.6667, Val AUPRC: 0.3559\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.5348, Val Loss: 0.6863\n",
      "Val AUROC: 0.6838, Val AUPRC: 0.3045\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.5222, Val Loss: 0.6739\n",
      "Val AUROC: 0.6912, Val AUPRC: 0.3079\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.5092, Val Loss: 0.7067\n",
      "Val AUROC: 0.6912, Val AUPRC: 0.2687\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.5015\n",
      "accuracy: 0.7627\n",
      "auprc: 0.5486\n",
      "auroc: 0.8222\n",
      "\n",
      "New best parameters found!\n",
      "New best combined score: 0.6949\n",
      "AUROC: 0.8222\n",
      "AUPRC: 0.5486\n",
      "Accuracy: 0.7627\n",
      "\n",
      "Trying combination 7/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=16, class_weights=(1.0, 8.5), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6902, Val Loss: 0.6893\n",
      "Val AUROC: 0.6936, Val AUPRC: 0.3346\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6905, Val Loss: 0.6895\n",
      "Val AUROC: 0.6642, Val AUPRC: 0.3432\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6863, Val Loss: 0.6891\n",
      "Val AUROC: 0.6422, Val AUPRC: 0.3440\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6895, Val Loss: 0.6892\n",
      "Val AUROC: 0.6225, Val AUPRC: 0.3404\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6859, Val Loss: 0.6881\n",
      "Val AUROC: 0.6127, Val AUPRC: 0.3342\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6860, Val Loss: 0.6884\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.3519\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6819, Val Loss: 0.6926\n",
      "Val AUROC: 0.6103, Val AUPRC: 0.3922\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6821, Val Loss: 0.6840\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.3933\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6752, Val Loss: 0.6921\n",
      "Val AUROC: 0.6103, Val AUPRC: 0.3513\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6830, Val Loss: 0.6847\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.3304\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6793, Val Loss: 0.6840\n",
      "Val AUROC: 0.6054, Val AUPRC: 0.3299\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6776, Val Loss: 0.6825\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.3304\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6731, Val Loss: 0.6875\n",
      "Val AUROC: 0.6029, Val AUPRC: 0.2554\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6645, Val Loss: 0.6944\n",
      "Val AUROC: 0.6029, Val AUPRC: 0.2480\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6653, Val Loss: 0.6858\n",
      "Val AUROC: 0.6029, Val AUPRC: 0.2480\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.6536, Val Loss: 0.6939\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.3104\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.6605, Val Loss: 0.6830\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.3050\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.6472, Val Loss: 0.6712\n",
      "Val AUROC: 0.6250, Val AUPRC: 0.3150\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.6446, Val Loss: 0.6734\n",
      "Val AUROC: 0.6495, Val AUPRC: 0.3377\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.6269, Val Loss: 0.6638\n",
      "Val AUROC: 0.6495, Val AUPRC: 0.3299\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.6040\n",
      "accuracy: 0.7966\n",
      "auprc: 0.2553\n",
      "auroc: 0.7333\n",
      "\n",
      "Trying combination 8/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.0001, batch_size=16, class_weights=(1.0, 8.5), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6908, Val Loss: 0.6910\n",
      "Val AUROC: 0.5294, Val AUPRC: 0.2267\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6901, Val Loss: 0.6981\n",
      "Val AUROC: 0.5466, Val AUPRC: 0.2371\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6914, Val Loss: 0.6980\n",
      "Val AUROC: 0.5441, Val AUPRC: 0.2366\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6956, Val Loss: 0.6966\n",
      "Val AUROC: 0.5564, Val AUPRC: 0.2449\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6924, Val Loss: 0.6928\n",
      "Val AUROC: 0.5613, Val AUPRC: 0.2515\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6885, Val Loss: 0.6871\n",
      "Val AUROC: 0.5735, Val AUPRC: 0.2721\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6891, Val Loss: 0.6957\n",
      "Val AUROC: 0.5833, Val AUPRC: 0.2801\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6857, Val Loss: 0.6928\n",
      "Val AUROC: 0.5882, Val AUPRC: 0.2824\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.6867, Val Loss: 0.6954\n",
      "Val AUROC: 0.5980, Val AUPRC: 0.2881\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.6875, Val Loss: 0.6882\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.3051\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.6782, Val Loss: 0.6843\n",
      "Val AUROC: 0.6103, Val AUPRC: 0.3064\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.6786, Val Loss: 0.6814\n",
      "Val AUROC: 0.6275, Val AUPRC: 0.3631\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.6748, Val Loss: 0.6751\n",
      "Val AUROC: 0.6569, Val AUPRC: 0.2980\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.6596, Val Loss: 0.6463\n",
      "Val AUROC: 0.6814, Val AUPRC: 0.3072\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.6420, Val Loss: 0.6452\n",
      "Val AUROC: 0.7132, Val AUPRC: 0.3115\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.6092, Val Loss: 0.6151\n",
      "Val AUROC: 0.7010, Val AUPRC: 0.2912\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.5723, Val Loss: 0.6138\n",
      "Val AUROC: 0.7083, Val AUPRC: 0.2758\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.5468, Val Loss: 0.6110\n",
      "Val AUROC: 0.7181, Val AUPRC: 0.2710\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.5213, Val Loss: 0.6464\n",
      "Val AUROC: 0.7230, Val AUPRC: 0.2791\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.5167, Val Loss: 0.6440\n",
      "Val AUROC: 0.7328, Val AUPRC: 0.2834\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.4399\n",
      "accuracy: 0.7627\n",
      "auprc: 0.5407\n",
      "auroc: 0.8926\n",
      "\n",
      "New best parameters found!\n",
      "New best combined score: 0.7129\n",
      "AUROC: 0.8926\n",
      "AUPRC: 0.5407\n",
      "Accuracy: 0.7627\n",
      "\n",
      "Trying combination 9/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=64, class_weights=(1.0, 7.143), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6973, Val Loss: 0.6976\n",
      "Val AUROC: 0.4534, Val AUPRC: 0.1286\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6870, Val Loss: 0.6948\n",
      "Val AUROC: 0.5000, Val AUPRC: 0.1489\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6827, Val Loss: 0.6924\n",
      "Val AUROC: 0.5417, Val AUPRC: 0.1672\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6747, Val Loss: 0.6910\n",
      "Val AUROC: 0.5711, Val AUPRC: 0.1916\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6647, Val Loss: 0.6870\n",
      "Val AUROC: 0.5956, Val AUPRC: 0.2070\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6456, Val Loss: 0.6804\n",
      "Val AUROC: 0.6176, Val AUPRC: 0.2483\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6241, Val Loss: 0.6605\n",
      "Val AUROC: 0.6495, Val AUPRC: 0.3409\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.5878, Val Loss: 0.6756\n",
      "Val AUROC: 0.6642, Val AUPRC: 0.2984\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.5864, Val Loss: 0.6827\n",
      "Val AUROC: 0.6814, Val AUPRC: 0.3101\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.4919, Val Loss: 0.6531\n",
      "Val AUROC: 0.7108, Val AUPRC: 0.2947\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.4863, Val Loss: 0.6910\n",
      "Val AUROC: 0.7108, Val AUPRC: 0.2776\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.4673, Val Loss: 0.7296\n",
      "Val AUROC: 0.7181, Val AUPRC: 0.2924\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.4751, Val Loss: 0.7137\n",
      "Val AUROC: 0.7132, Val AUPRC: 0.2968\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.4657, Val Loss: 0.6670\n",
      "Val AUROC: 0.7255, Val AUPRC: 0.2977\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.4230, Val Loss: 0.6563\n",
      "Val AUROC: 0.7255, Val AUPRC: 0.2815\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.3945, Val Loss: 0.6806\n",
      "Val AUROC: 0.7206, Val AUPRC: 0.2915\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.3847, Val Loss: 0.7184\n",
      "Val AUROC: 0.7279, Val AUPRC: 0.2961\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.3745, Val Loss: 0.8313\n",
      "Val AUROC: 0.7181, Val AUPRC: 0.2868\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.3641, Val Loss: 0.7526\n",
      "Val AUROC: 0.7255, Val AUPRC: 0.2762\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.3450, Val Loss: 0.7766\n",
      "Val AUROC: 0.7426, Val AUPRC: 0.2845\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered at epoch 19\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.2641\n",
      "accuracy: 0.8305\n",
      "auprc: 0.7226\n",
      "auroc: 0.9704\n",
      "\n",
      "New best parameters found!\n",
      "New best combined score: 0.8293\n",
      "AUROC: 0.9704\n",
      "AUPRC: 0.7226\n",
      "Accuracy: 0.8305\n",
      "\n",
      "Trying combination 10/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=64, class_weights=(1.0, 7.143), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6913, Val Loss: 0.6865\n",
      "Val AUROC: 0.5760, Val AUPRC: 0.3279\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6850, Val Loss: 0.6825\n",
      "Val AUROC: 0.6299, Val AUPRC: 0.2418\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6748, Val Loss: 0.6745\n",
      "Val AUROC: 0.6544, Val AUPRC: 0.3282\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6628, Val Loss: 0.6575\n",
      "Val AUROC: 0.6544, Val AUPRC: 0.4021\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6330, Val Loss: 0.6390\n",
      "Val AUROC: 0.6814, Val AUPRC: 0.4104\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.5736, Val Loss: 0.6308\n",
      "Val AUROC: 0.7034, Val AUPRC: 0.3152\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.5331, Val Loss: 0.6340\n",
      "Val AUROC: 0.7304, Val AUPRC: 0.2906\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.5012, Val Loss: 0.6561\n",
      "Val AUROC: 0.7206, Val AUPRC: 0.2526\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.4677, Val Loss: 0.6377\n",
      "Val AUROC: 0.7304, Val AUPRC: 0.2893\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.4563, Val Loss: 0.6171\n",
      "Val AUROC: 0.7353, Val AUPRC: 0.2944\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.4232, Val Loss: 0.5901\n",
      "Val AUROC: 0.7500, Val AUPRC: 0.2851\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.4127, Val Loss: 0.6279\n",
      "Val AUROC: 0.7451, Val AUPRC: 0.2750\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.3921, Val Loss: 0.6735\n",
      "Val AUROC: 0.7377, Val AUPRC: 0.2568\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.3758, Val Loss: 0.6939\n",
      "Val AUROC: 0.7230, Val AUPRC: 0.2535\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.3468, Val Loss: 0.8283\n",
      "Val AUROC: 0.7402, Val AUPRC: 0.2595\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.3334, Val Loss: 0.7130\n",
      "Val AUROC: 0.7402, Val AUPRC: 0.2713\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.2954, Val Loss: 1.1368\n",
      "Val AUROC: 0.7206, Val AUPRC: 0.2574\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.4256, Val Loss: 0.8565\n",
      "Val AUROC: 0.7206, Val AUPRC: 0.2634\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.2791, Val Loss: 1.0455\n",
      "Val AUROC: 0.7181, Val AUPRC: 0.2479\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.2918, Val Loss: 0.8045\n",
      "Val AUROC: 0.7157, Val AUPRC: 0.2580\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.3251\n",
      "accuracy: 0.7966\n",
      "auprc: 0.6548\n",
      "auroc: 0.9444\n",
      "\n",
      "Trying combination 11/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=64, class_weights=(1.0, 8.5), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6944, Val Loss: 0.6855\n",
      "Val AUROC: 0.6054, Val AUPRC: 0.1722\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6860, Val Loss: 0.6781\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.2018\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6777, Val Loss: 0.6763\n",
      "Val AUROC: 0.6005, Val AUPRC: 0.2089\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6773, Val Loss: 0.6749\n",
      "Val AUROC: 0.6005, Val AUPRC: 0.2174\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6745, Val Loss: 0.6757\n",
      "Val AUROC: 0.5833, Val AUPRC: 0.2031\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6650, Val Loss: 0.6730\n",
      "Val AUROC: 0.6029, Val AUPRC: 0.2231\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.6580, Val Loss: 0.6647\n",
      "Val AUROC: 0.6324, Val AUPRC: 0.3209\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.6280, Val Loss: 0.6327\n",
      "Val AUROC: 0.6961, Val AUPRC: 0.3265\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.5508, Val Loss: 0.6479\n",
      "Val AUROC: 0.7059, Val AUPRC: 0.3317\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.5134, Val Loss: 0.7106\n",
      "Val AUROC: 0.7230, Val AUPRC: 0.3204\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.5005, Val Loss: 0.6919\n",
      "Val AUROC: 0.7181, Val AUPRC: 0.2878\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.4744, Val Loss: 0.6228\n",
      "Val AUROC: 0.7328, Val AUPRC: 0.3179\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.4418, Val Loss: 0.6692\n",
      "Val AUROC: 0.7353, Val AUPRC: 0.3166\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.4033, Val Loss: 0.7120\n",
      "Val AUROC: 0.7353, Val AUPRC: 0.3206\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.3836, Val Loss: 0.7869\n",
      "Val AUROC: 0.7353, Val AUPRC: 0.3040\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.3699, Val Loss: 0.7624\n",
      "Val AUROC: 0.7304, Val AUPRC: 0.3134\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.3705, Val Loss: 0.8077\n",
      "Val AUROC: 0.7328, Val AUPRC: 0.2990\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.3727, Val Loss: 0.8001\n",
      "Val AUROC: 0.7328, Val AUPRC: 0.3176\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.3208, Val Loss: 0.8231\n",
      "Val AUROC: 0.7377, Val AUPRC: 0.3527\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.3413, Val Loss: 0.8405\n",
      "Val AUROC: 0.7475, Val AUPRC: 0.3039\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.2946\n",
      "accuracy: 0.8136\n",
      "auprc: 0.4776\n",
      "auroc: 0.9370\n",
      "\n",
      "Trying combination 12/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=64, class_weights=(1.0, 8.5), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6982, Val Loss: 0.6878\n",
      "Val AUROC: 0.6373, Val AUPRC: 0.2587\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6861, Val Loss: 0.6824\n",
      "Val AUROC: 0.6029, Val AUPRC: 0.2410\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6798, Val Loss: 0.6752\n",
      "Val AUROC: 0.6201, Val AUPRC: 0.2200\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6649, Val Loss: 0.6694\n",
      "Val AUROC: 0.6275, Val AUPRC: 0.2286\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.6467, Val Loss: 0.6593\n",
      "Val AUROC: 0.6324, Val AUPRC: 0.2506\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6109, Val Loss: 0.6648\n",
      "Val AUROC: 0.6103, Val AUPRC: 0.2318\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.5557, Val Loss: 0.6586\n",
      "Val AUROC: 0.7034, Val AUPRC: 0.2645\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.4817, Val Loss: 0.6239\n",
      "Val AUROC: 0.7328, Val AUPRC: 0.2966\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.4719, Val Loss: 0.6149\n",
      "Val AUROC: 0.7475, Val AUPRC: 0.3102\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.4454, Val Loss: 0.7091\n",
      "Val AUROC: 0.7475, Val AUPRC: 0.3196\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.4312, Val Loss: 0.6076\n",
      "Val AUROC: 0.7475, Val AUPRC: 0.3189\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.4034, Val Loss: 0.6241\n",
      "Val AUROC: 0.7868, Val AUPRC: 0.3434\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.4629, Val Loss: 0.5694\n",
      "Val AUROC: 0.7745, Val AUPRC: 0.3415\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.3798, Val Loss: 0.5904\n",
      "Val AUROC: 0.7672, Val AUPRC: 0.3304\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.3620, Val Loss: 0.8106\n",
      "Val AUROC: 0.7647, Val AUPRC: 0.3287\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.3756, Val Loss: 0.7530\n",
      "Val AUROC: 0.7475, Val AUPRC: 0.3295\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.3732, Val Loss: 0.8416\n",
      "Val AUROC: 0.7745, Val AUPRC: 0.4090\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.3759, Val Loss: 0.6256\n",
      "Val AUROC: 0.7794, Val AUPRC: 0.3453\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.3151, Val Loss: 0.6813\n",
      "Val AUROC: 0.7770, Val AUPRC: 0.3416\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.3195, Val Loss: 0.5995\n",
      "Val AUROC: 0.7745, Val AUPRC: 0.3925\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.3193\n",
      "accuracy: 0.7966\n",
      "auprc: 0.7209\n",
      "auroc: 0.9519\n",
      "\n",
      "Trying combination 13/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=16, class_weights=(1.0, 7.143), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6916, Val Loss: 0.6865\n",
      "Val AUROC: 0.6225, Val AUPRC: 0.3104\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6801, Val Loss: 0.6918\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.2989\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6565, Val Loss: 0.6776\n",
      "Val AUROC: 0.6275, Val AUPRC: 0.3084\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.5980, Val Loss: 0.6552\n",
      "Val AUROC: 0.6961, Val AUPRC: 0.3083\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.5758, Val Loss: 0.6397\n",
      "Val AUROC: 0.7353, Val AUPRC: 0.3189\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.5350, Val Loss: 0.6708\n",
      "Val AUROC: 0.6838, Val AUPRC: 0.2535\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.4592, Val Loss: 0.8226\n",
      "Val AUROC: 0.6642, Val AUPRC: 0.2352\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.4971, Val Loss: 0.7718\n",
      "Val AUROC: 0.6569, Val AUPRC: 0.2319\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.4364, Val Loss: 0.8495\n",
      "Val AUROC: 0.6887, Val AUPRC: 0.2483\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.5345, Val Loss: 0.7691\n",
      "Val AUROC: 0.7108, Val AUPRC: 0.2773\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.4602, Val Loss: 0.8839\n",
      "Val AUROC: 0.6740, Val AUPRC: 0.2430\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.4184, Val Loss: 0.9241\n",
      "Val AUROC: 0.7181, Val AUPRC: 0.2634\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.4534, Val Loss: 0.7825\n",
      "Val AUROC: 0.6985, Val AUPRC: 0.2489\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.3980, Val Loss: 0.8743\n",
      "Val AUROC: 0.7255, Val AUPRC: 0.2850\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.3521, Val Loss: 0.8902\n",
      "Val AUROC: 0.7230, Val AUPRC: 0.2563\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping triggered at epoch 14\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.3201\n",
      "accuracy: 0.8983\n",
      "auprc: 0.5998\n",
      "auroc: 0.9444\n",
      "\n",
      "Trying combination 14/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=16, class_weights=(1.0, 7.143), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6931, Val Loss: 0.6942\n",
      "Val AUROC: 0.5637, Val AUPRC: 0.3130\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6793, Val Loss: 0.6642\n",
      "Val AUROC: 0.6152, Val AUPRC: 0.3121\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.5999, Val Loss: 0.6568\n",
      "Val AUROC: 0.7132, Val AUPRC: 0.2756\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.5344, Val Loss: 0.6822\n",
      "Val AUROC: 0.7279, Val AUPRC: 0.2614\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.5398, Val Loss: 0.6097\n",
      "Val AUROC: 0.7525, Val AUPRC: 0.3136\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.5600, Val Loss: 0.6588\n",
      "Val AUROC: 0.7377, Val AUPRC: 0.3104\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.4782, Val Loss: 0.6915\n",
      "Val AUROC: 0.7206, Val AUPRC: 0.2744\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.4576, Val Loss: 0.6759\n",
      "Val AUROC: 0.7500, Val AUPRC: 0.3164\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.4433, Val Loss: 0.7685\n",
      "Val AUROC: 0.7574, Val AUPRC: 0.3092\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.4123, Val Loss: 0.6859\n",
      "Val AUROC: 0.7721, Val AUPRC: 0.3138\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.4453, Val Loss: 0.9565\n",
      "Val AUROC: 0.7794, Val AUPRC: 0.3024\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.4235, Val Loss: 0.5990\n",
      "Val AUROC: 0.7941, Val AUPRC: 0.3521\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.3847, Val Loss: 0.7260\n",
      "Val AUROC: 0.8113, Val AUPRC: 0.3255\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.3384, Val Loss: 1.0864\n",
      "Val AUROC: 0.7966, Val AUPRC: 0.3096\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.3859, Val Loss: 0.7376\n",
      "Val AUROC: 0.8088, Val AUPRC: 0.3468\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.3353, Val Loss: 1.2419\n",
      "Val AUROC: 0.7255, Val AUPRC: 0.2912\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.3718, Val Loss: 0.6329\n",
      "Val AUROC: 0.8235, Val AUPRC: 0.4935\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.2633, Val Loss: 0.9447\n",
      "Val AUROC: 0.7917, Val AUPRC: 0.3787\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.2855, Val Loss: 0.8466\n",
      "Val AUROC: 0.7941, Val AUPRC: 0.4731\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.2369, Val Loss: 1.3095\n",
      "Val AUROC: 0.7892, Val AUPRC: 0.2923\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.4752\n",
      "accuracy: 0.9153\n",
      "auprc: 0.7477\n",
      "auroc: 0.9370\n",
      "\n",
      "New best parameters found!\n",
      "New best combined score: 0.8548\n",
      "AUROC: 0.9370\n",
      "AUPRC: 0.7477\n",
      "Accuracy: 0.9153\n",
      "\n",
      "Trying combination 15/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=16, class_weights=(1.0, 8.5), bidirectional=False\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6931, Val Loss: 0.6833\n",
      "Val AUROC: 0.6446, Val AUPRC: 0.2451\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6774, Val Loss: 0.6724\n",
      "Val AUROC: 0.6618, Val AUPRC: 0.3434\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.6689, Val Loss: 0.6510\n",
      "Val AUROC: 0.6789, Val AUPRC: 0.3519\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.6087, Val Loss: 0.6501\n",
      "Val AUROC: 0.7157, Val AUPRC: 0.3724\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.5686, Val Loss: 0.6564\n",
      "Val AUROC: 0.7059, Val AUPRC: 0.2913\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 6/20\n",
      "Train Loss: 0.6330, Val Loss: 0.6905\n",
      "Val AUROC: 0.6765, Val AUPRC: 0.3034\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 7/20\n",
      "Train Loss: 0.5184, Val Loss: 0.7187\n",
      "Val AUROC: 0.6789, Val AUPRC: 0.2618\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 8/20\n",
      "Train Loss: 0.4627, Val Loss: 0.7256\n",
      "Val AUROC: 0.6618, Val AUPRC: 0.2654\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 9/20\n",
      "Train Loss: 0.4690, Val Loss: 0.7546\n",
      "Val AUROC: 0.6789, Val AUPRC: 0.2513\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 10/20\n",
      "Train Loss: 0.4200, Val Loss: 0.7783\n",
      "Val AUROC: 0.7108, Val AUPRC: 0.2603\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 11/20\n",
      "Train Loss: 0.4437, Val Loss: 0.6304\n",
      "Val AUROC: 0.7451, Val AUPRC: 0.2785\n",
      "\n",
      "Split 1, Epoch 12/20\n",
      "Train Loss: 0.3954, Val Loss: 0.7333\n",
      "Val AUROC: 0.7475, Val AUPRC: 0.2680\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 13/20\n",
      "Train Loss: 0.4227, Val Loss: 0.9544\n",
      "Val AUROC: 0.7132, Val AUPRC: 0.2683\n",
      "EarlyStopping counter: 2 out of 10\n",
      "\n",
      "Split 1, Epoch 14/20\n",
      "Train Loss: 0.4436, Val Loss: 0.8663\n",
      "Val AUROC: 0.7108, Val AUPRC: 0.2532\n",
      "EarlyStopping counter: 3 out of 10\n",
      "\n",
      "Split 1, Epoch 15/20\n",
      "Train Loss: 0.3973, Val Loss: 0.7778\n",
      "Val AUROC: 0.7672, Val AUPRC: 0.2947\n",
      "EarlyStopping counter: 4 out of 10\n",
      "\n",
      "Split 1, Epoch 16/20\n",
      "Train Loss: 0.3904, Val Loss: 0.9150\n",
      "Val AUROC: 0.7255, Val AUPRC: 0.2612\n",
      "EarlyStopping counter: 5 out of 10\n",
      "\n",
      "Split 1, Epoch 17/20\n",
      "Train Loss: 0.3415, Val Loss: 0.8302\n",
      "Val AUROC: 0.7451, Val AUPRC: 0.2619\n",
      "EarlyStopping counter: 6 out of 10\n",
      "\n",
      "Split 1, Epoch 18/20\n",
      "Train Loss: 0.3505, Val Loss: 1.0580\n",
      "Val AUROC: 0.6961, Val AUPRC: 0.2581\n",
      "EarlyStopping counter: 7 out of 10\n",
      "\n",
      "Split 1, Epoch 19/20\n",
      "Train Loss: 0.3220, Val Loss: 1.1559\n",
      "Val AUROC: 0.6642, Val AUPRC: 0.2595\n",
      "EarlyStopping counter: 8 out of 10\n",
      "\n",
      "Split 1, Epoch 20/20\n",
      "Train Loss: 0.3737, Val Loss: 1.2534\n",
      "Val AUROC: 0.6642, Val AUPRC: 0.2483\n",
      "EarlyStopping counter: 9 out of 10\n",
      "\n",
      "Split 1 Test Metrics:\n",
      "loss: 0.4429\n",
      "accuracy: 0.8814\n",
      "auprc: 0.5436\n",
      "auroc: 0.9111\n",
      "\n",
      "Trying combination 16/128:\n",
      "Parameters: hidden_size=32, num_layers=2, dropout=0.2, lr=0.001, batch_size=16, class_weights=(1.0, 8.5), bidirectional=True\n",
      "\n",
      "=== Training on Split 1/1 ===\n",
      "\n",
      "Split 1, Epoch 1/20\n",
      "Train Loss: 0.6931, Val Loss: 0.6886\n",
      "Val AUROC: 0.6078, Val AUPRC: 0.2166\n",
      "\n",
      "Split 1, Epoch 2/20\n",
      "Train Loss: 0.6865, Val Loss: 0.6948\n",
      "Val AUROC: 0.6397, Val AUPRC: 0.4139\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 3/20\n",
      "Train Loss: 0.7070, Val Loss: 0.6319\n",
      "Val AUROC: 0.7083, Val AUPRC: 0.3954\n",
      "\n",
      "Split 1, Epoch 4/20\n",
      "Train Loss: 0.5929, Val Loss: 0.8018\n",
      "Val AUROC: 0.6667, Val AUPRC: 0.3245\n",
      "EarlyStopping counter: 1 out of 10\n",
      "\n",
      "Split 1, Epoch 5/20\n",
      "Train Loss: 0.5246, Val Loss: 0.7175\n",
      "Val AUROC: 0.7157, Val AUPRC: 0.2622\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m reload(train)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m grid_search\n\u001b[1;32m----> 6\u001b[0m best_params_grid_search, best_metrics_grid_search, all_results_grid_search \u001b[38;5;241m=\u001b[39m grid_search(n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m,param_grid \u001b[38;5;241m=\u001b[39m unique_values, split_number\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\train.py:522\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(n_epochs, param_grid, split_number)\u001b[0m\n\u001b[0;32m    500\u001b[0m base_model_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m37\u001b[39m,\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatic_input_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbidirectional\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    506\u001b[0m }\n\u001b[0;32m    508\u001b[0m base_training_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: n_epochs,\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weights\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1.163\u001b[39m, \u001b[38;5;241m7.143\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m     }\n\u001b[0;32m    520\u001b[0m }\n\u001b[1;32m--> 522\u001b[0m best_params, best_metrics, all_results \u001b[38;5;241m=\u001b[39m train_with_grid_search(\n\u001b[0;32m    523\u001b[0m     DSSM,\n\u001b[0;32m    524\u001b[0m     base_model_params,\n\u001b[0;32m    525\u001b[0m     base_training_params,\n\u001b[0;32m    526\u001b[0m     device,\n\u001b[0;32m    527\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[0;32m    528\u001b[0m     split_number\u001b[38;5;241m=\u001b[39msplit_number\n\u001b[0;32m    529\u001b[0m )\n\u001b[0;32m    531\u001b[0m results_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_outputs/dssm_output/grid_search_results_with_bidirection.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\train.py:397\u001b[0m, in \u001b[0;36mtrain_with_grid_search\u001b[1;34m(model_class, base_model_params, base_training_params, device, param_grid, split_number)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrying combination \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_combinations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters: hidden_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_layers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    394\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdropout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_weights=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_weights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, bidirectional=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbidirectional\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 397\u001b[0m metrics, splits_metrics \u001b[38;5;241m=\u001b[39m train_cross_validation(\n\u001b[0;32m    398\u001b[0m     model_class\u001b[38;5;241m=\u001b[39mmodel_class,\n\u001b[0;32m    399\u001b[0m     model_params\u001b[38;5;241m=\u001b[39mmodel_params,\n\u001b[0;32m    400\u001b[0m     training_params\u001b[38;5;241m=\u001b[39mtraining_params,\n\u001b[0;32m    401\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    402\u001b[0m     split_number\u001b[38;5;241m=\u001b[39msplit_number\n\u001b[0;32m    403\u001b[0m )\n\u001b[0;32m    405\u001b[0m combined_score \u001b[38;5;241m=\u001b[39m calculate_combined_score(metrics)\n\u001b[0;32m    406\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m combined_score\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\train.py:284\u001b[0m, in \u001b[0;36mtrain_cross_validation\u001b[1;34m(model_class, model_params, training_params, device, split_number)\u001b[0m\n\u001b[0;32m    279\u001b[0m model, dataloaders, criterion, optimizer \u001b[38;5;241m=\u001b[39m initialize_training(\n\u001b[0;32m    280\u001b[0m     model_class, model_params, training_params, split, device\n\u001b[0;32m    281\u001b[0m )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Train on this split\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m split_metrics \u001b[38;5;241m=\u001b[39m train_split(\n\u001b[0;32m    285\u001b[0m     split, model, dataloaders, criterion, optimizer,\n\u001b[0;32m    286\u001b[0m     training_params, device\n\u001b[0;32m    287\u001b[0m )\n\u001b[0;32m    288\u001b[0m splits_metrics\u001b[38;5;241m.\u001b[39mappend(split_metrics)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# Save intermediate results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\train.py:204\u001b[0m, in \u001b[0;36mtrain_split\u001b[1;34m(split, model, dataloaders, criterion, optimizer, training_params, device)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m    200\u001b[0m     train_metrics \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[0;32m    201\u001b[0m         model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], optimizer, criterion, device, tracker\n\u001b[0;32m    202\u001b[0m     )\n\u001b[1;32m--> 204\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[0;32m    205\u001b[0m         model, dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m], criterion, device, tracker\n\u001b[0;32m    206\u001b[0m     )\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSplit \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\train.py:122\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, loader, criterion, device, tracker)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    118\u001b[0m     temporal_batch, static_batch, target_batch, seq_lengths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    119\u001b[0m         x\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch\n\u001b[0;32m    120\u001b[0m     ]\n\u001b[1;32m--> 122\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(temporal_batch, static_batch, seq_lengths)\n\u001b[0;32m    123\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target_batch)\n\u001b[0;32m    125\u001b[0m     tracker\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), outputs, target_batch, temporal_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\s233148\\.conda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s233148\\.conda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\models\\dssm.py:37\u001b[0m, in \u001b[0;36mDSSM.forward\u001b[1;34m(self, temporal_data, static_data, seq_lengths)\u001b[0m\n\u001b[0;32m     34\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m temporal_data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Process temporal and static data\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m temporal_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_encoder(temporal_data, seq_lengths)  \u001b[38;5;66;03m# Shape: [batch, seq_len, hidden]\u001b[39;00m\n\u001b[0;32m     38\u001b[0m static_repr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_encoder(static_data)  \u001b[38;5;66;03m# Shape: [batch, hidden]\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Get final temporal representation (use the last relevant timestep for each sequence)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s233148\\.conda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s233148\\.conda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Repos\\SSM_EHR_Classification\\models\\dssm.py:79\u001b[0m, in \u001b[0;36mTemporalEncoder.forward\u001b[1;34m(self, temporal_data, seq_lengths)\u001b[0m\n\u001b[0;32m     74\u001b[0m packed_input \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(\n\u001b[0;32m     75\u001b[0m     temporal_data, seq_lengths\u001b[38;5;241m.\u001b[39mcpu(), batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Process with LSTM\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m packed_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(packed_input)\n\u001b[0;32m     80\u001b[0m lstm_output, _ \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_packed_sequence(packed_output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Apply attention\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s233148\\.conda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\s233148\\.conda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\s233148\\.conda\\envs\\ML\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1135\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1125\u001b[0m         hx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m   1133\u001b[0m     )\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[0;32m   1138\u001b[0m         hx,\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[0;32m   1146\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1147\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import train\n",
    "from importlib import reload\n",
    "reload(train)\n",
    "from train import grid_search\n",
    "\n",
    "best_params_grid_search, best_metrics_grid_search, all_results_grid_search = grid_search(n_epochs = 20,param_grid = unique_values, split_number=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of the grid search:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_results_grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults of the grid search:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m show_results(all_results_grid_search)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_score\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_results_grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Results of the grid search:\")\n",
    "df = show_results(all_results_grid_search)\n",
    "df.sort_values(\"combined_score\", ascending=False)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Prepare the data\n",
    "data = {\n",
    "    'grud': {\n",
    "        \"loss\": 0.4254, \n",
    "        \"accuracy\": 0.7873, \n",
    "        \"auprc\": 0.5281, \n",
    "        \"auroc\": 0.8636\n",
    "    },\n",
    "    'ipnets': {\n",
    "        \"loss\": 0.4416, \n",
    "        \"accuracy\": 0.7802, \n",
    "        \"auprc\": 0.5132, \n",
    "        \"auroc\": 0.8499\n",
    "    },\n",
    "    'seft': {\n",
    "        \"loss\": 0.4024, \n",
    "        \"accuracy\": 0.801, \n",
    "        \"auprc\": 0.5339, \n",
    "        \"auroc\": 0.8585\n",
    "    },\n",
    "    'transformer': {\n",
    "        \"loss\": 0.3904, \n",
    "        \"accuracy\": 0.8182, \n",
    "        \"auprc\": 0.5202, \n",
    "        \"auroc\": 0.8612\n",
    "    },\n",
    "    'DSSM': {\n",
    "       \"loss\": df['loss'][0],\n",
    "        \"accuracy\": df['accuracy'][0],\n",
    "        \"auprc\": df['auprc'][0],\n",
    "        \"auroc\": df['auroc'][0],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Metrics where lower is better\n",
    "lower_better = ['loss']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data).T\n",
    "\n",
    "# Normalize the data\n",
    "def normalize_data(df, lower_better_cols):\n",
    "    normalized_df = df.copy()\n",
    "    for col in df.columns:\n",
    "        if col in lower_better_cols:\n",
    "            # For lower better metrics, invert and min-max normalize\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            normalized_df[col] = 1 - (df[col] - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            # For higher better metrics, do standard min-max normalization\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            normalized_df[col] = (df[col] - col_min) / (col_max - col_min)\n",
    "    return normalized_df\n",
    "\n",
    "# Normalize data\n",
    "normalized_df = normalize_data(df, lower_better)\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(normalized_df, annot=df, cmap='YlGnBu', \n",
    "            fmt='.4f', cbar_kws={'label': 'Normalized Performance'})\n",
    "\n",
    "plt.title('Normalized Model Performance Metrics', fontsize=16)\n",
    "plt.ylabel('Models', fontsize=12)\n",
    "plt.xlabel('Metrics', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
