{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8cd74a1c",
      "metadata": {
        "id": "8cd74a1c"
      },
      "source": [
        "## General notes on model implementation:\n",
        "\n",
        "Data Preprocessing:\n",
        "\n",
        "Devide data into:\n",
        "\n",
        "-Temporal data: Two stamp measurements over time\n",
        "-Static data: Patient-specific information such as age, height, and gender.\n",
        "-Target: The variable you want to predict (classification)\n",
        "\n",
        "\n",
        "Model Architecture:\n",
        "\n",
        "-Recurrent Layer (for time-dependent sensor data)\n",
        "-Feed-forward Layer (for static features and final output)\n",
        "-Combination Layer: Combines the output of the LSTM and static features into a unified representation.\n",
        "-Output Layer: Generates the final prediction for each timestamp.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e2b3b2",
      "metadata": {
        "id": "64e2b3b2"
      },
      "source": [
        "# Classification problem using random data on our data shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c6b49e6",
      "metadata": {
        "id": "1c6b49e6",
        "outputId": "e7ba474c-e07d-46df-c9fe-b5d2078f9d7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 0.6942\n",
            "Epoch [2/10], Loss: 0.6717\n",
            "Epoch [3/10], Loss: 0.6647\n",
            "Epoch [4/10], Loss: 0.6608\n",
            "Epoch [5/10], Loss: 0.4475\n",
            "Epoch [6/10], Loss: 0.2163\n",
            "Epoch [7/10], Loss: 0.0688\n",
            "Epoch [8/10], Loss: 0.0069\n",
            "Epoch [9/10], Loss: 0.0053\n",
            "Epoch [10/10], Loss: 0.0030\n",
            "Sample prediction probabilities: tensor([[0.9896, 0.0104]])\n",
            "Predicted class: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Parameters for synthetic data\n",
        "num_patients = 1000             # Number of samples (patients)\n",
        "sequence_length = 10            # Length of time series per patient\n",
        "num_sensors = 2                 # Number of sensors (columns for timestamp measurements)\n",
        "num_measurements_per_sensor = 37 # Measurements per sensor per timestamp\n",
        "num_static_features = 8         # Static features (e.g., age, height, gender)\n",
        "num_classes = 2                 # Number of classes for classification\n",
        "\n",
        "# New temporal input size\n",
        "input_size = num_sensors * num_measurements_per_sensor  # 2 * 5 = 10\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Generate random synthetic data\n",
        "temporal_data = torch.randn(num_patients, sequence_length, input_size)   # Shape: (num_patients, sequence_length, input_size)\n",
        "static_data = torch.randn(num_patients, num_static_features)             # Shape: (num_patients, num_static_features)\n",
        "targets = torch.randint(0, num_classes, (num_patients,))                 # Shape: (num_patients,) with random class labels\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "dataset = TensorDataset(temporal_data, static_data, targets)\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the Deep State Space Model for Classification\n",
        "class DeepStateSpaceModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, static_input_size, num_classes, num_layers=1):\n",
        "        super(DeepStateSpaceModel, self).__init__()\n",
        "\n",
        "        # LSTM for temporal data\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Linear layer for static data\n",
        "        self.static_fc = nn.Linear(static_input_size, hidden_size)\n",
        "\n",
        "        # Combining LSTM output and static features\n",
        "        self.fc_combined = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "        # Output layer (for classification, matches num_classes)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, temporal_data, static_data):\n",
        "        # Pass temporal data through LSTM\n",
        "        lstm_out, _ = self.lstm(temporal_data)\n",
        "\n",
        "        # Take only the last hidden state (many-to-one structure)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Pass static data through linear layer\n",
        "        static_out = self.static_fc(static_data)\n",
        "\n",
        "        # Concatenate LSTM output with static output\n",
        "        combined = torch.cat((lstm_out, static_out), dim=1)\n",
        "\n",
        "        # Pass through a fully connected layer to combine features\n",
        "        combined_out = torch.relu(self.fc_combined(combined))\n",
        "\n",
        "        # Final output layer with no activation (use CrossEntropyLoss which applies softmax)\n",
        "        output = self.output_layer(combined_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = DeepStateSpaceModel(input_size=input_size, hidden_size=hidden_size,\n",
        "                            static_input_size=num_static_features, num_classes=num_classes, num_layers=num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # For classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        temporal_batch, static_batch, target_batch = batch\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(temporal_batch, static_batch)\n",
        "        loss = criterion(output, target_batch)  # CrossEntropyLoss expects raw logits\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Testing on a random sample\n",
        "with torch.no_grad():\n",
        "    sample_temporal = torch.randn(1, sequence_length, input_size)\n",
        "    sample_static = torch.randn(1, num_static_features)\n",
        "    prediction = model(sample_temporal, sample_static)\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probabilities = torch.softmax(prediction, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "    print(\"Sample prediction probabilities:\", probabilities)\n",
        "    print(\"Predicted class:\", predicted_class.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c842b89f",
      "metadata": {
        "id": "c842b89f"
      },
      "source": [
        "## Implementation on bigger dataset with accuracy metric\n",
        "\n",
        "Code with Train, Validation, and Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411fae37",
      "metadata": {
        "id": "411fae37",
        "outputId": "be192827-80bb-4ab3-b221-f6384b3b9063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.0997, Validation Accuracy: 34.00%\n",
            "Epoch [2/10], Loss: 1.1052, Validation Accuracy: 34.93%\n",
            "Epoch [3/10], Loss: 1.0651, Validation Accuracy: 36.00%\n",
            "Epoch [4/10], Loss: 1.1088, Validation Accuracy: 34.93%\n",
            "Epoch [5/10], Loss: 1.1247, Validation Accuracy: 36.27%\n",
            "Epoch [6/10], Loss: 1.0670, Validation Accuracy: 33.20%\n",
            "Epoch [7/10], Loss: 1.0449, Validation Accuracy: 34.93%\n",
            "Epoch [8/10], Loss: 1.0578, Validation Accuracy: 36.67%\n",
            "Epoch [9/10], Loss: 1.0921, Validation Accuracy: 34.13%\n",
            "Epoch [10/10], Loss: 1.0797, Validation Accuracy: 34.13%\n",
            "Test Accuracy: 40.67%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Parameters for synthetic data\n",
        "num_patients = 5000             # Total number of samples (patients)\n",
        "sequence_length = 10            # Length of time series per patient\n",
        "num_sensors = 2                 # Number of sensors\n",
        "num_measurements_per_sensor = 5 # Measurements per sensor per timestamp\n",
        "num_static_features = 3         # Static features (e.g., age, height, gender)\n",
        "num_classes = 3                 # Number of classes for classification\n",
        "\n",
        "# New temporal input size\n",
        "input_size = num_sensors * num_measurements_per_sensor  # 2 * 5 = 10\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Generate random synthetic data\n",
        "temporal_data = torch.randn(num_patients, sequence_length, input_size)   # Shape: (num_patients, sequence_length, input_size)\n",
        "static_data = torch.randn(num_patients, num_static_features)             # Shape: (num_patients, num_static_features)\n",
        "targets = torch.randint(0, num_classes, (num_patients,))                 # Shape: (num_patients,) with random class labels\n",
        "\n",
        "# Split dataset into train, validation, and test sets (70%, 15%, 15%)\n",
        "train_size = int(0.7 * num_patients)\n",
        "val_size = int(0.15 * num_patients)\n",
        "test_size = num_patients - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(TensorDataset(temporal_data, static_data, targets), [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define the Deep State Space Model for Classification\n",
        "class DeepStateSpaceModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, static_input_size, num_classes, num_layers=1):\n",
        "        super(DeepStateSpaceModel, self).__init__()\n",
        "\n",
        "        # LSTM for temporal data\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Linear layer for static data\n",
        "        self.static_fc = nn.Linear(static_input_size, hidden_size)\n",
        "\n",
        "        # Combining LSTM output and static features\n",
        "        self.fc_combined = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "        # Output layer (for classification, matches num_classes)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, temporal_data, static_data):\n",
        "        # Pass temporal data through LSTM\n",
        "        lstm_out, _ = self.lstm(temporal_data)\n",
        "\n",
        "        # Take only the last hidden state (many-to-one structure)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Pass static data through linear layer\n",
        "        static_out = self.static_fc(static_data)\n",
        "\n",
        "        # Concatenate LSTM output with static output\n",
        "        combined = torch.cat((lstm_out, static_out), dim=1)\n",
        "\n",
        "        # Pass through a fully connected layer to combine features\n",
        "        combined_out = torch.relu(self.fc_combined(combined))\n",
        "\n",
        "        # Final output layer with no activation (use CrossEntropyLoss which applies softmax)\n",
        "        output = self.output_layer(combined_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = DeepStateSpaceModel(input_size=input_size, hidden_size=hidden_size,\n",
        "                            static_input_size=num_static_features, num_classes=num_classes, num_layers=num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # For classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with validation\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch in train_loader:\n",
        "        temporal_batch, static_batch, target_batch = batch\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(temporal_batch, static_batch)\n",
        "        loss = criterion(output, target_batch)  # CrossEntropyLoss expects raw logits\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop to calculate accuracy on validation data\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            temporal_batch, static_batch, target_batch = batch\n",
        "            output = model(temporal_batch, static_batch)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(output, 1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Final test accuracy calculation\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        temporal_batch, static_batch, target_batch = batch\n",
        "        output = model(temporal_batch, static_batch)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(output, 1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88a6e48e",
      "metadata": {
        "id": "88a6e48e"
      },
      "source": [
        "## Implementation with making timestamp number a dynamic variable depending on each patient\n",
        "\n",
        "Dynamic Padding: Use padding to ensure each batch has a uniform sequence length, required by LSTM layers. We'll use torch.nn.utils.rnn.pad_sequence for padding sequences within a batch.\n",
        "Pack Padded Sequences: Use torch.nn.utils.rnn.pack_padded_sequence and pad_packed_sequence within the LSTM model to handle variable-length sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f174a51",
      "metadata": {
        "id": "5f174a51",
        "outputId": "4005d0ac-604c-4d40-8573-bc0ababae915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.1072, Validation Accuracy: 31.47%\n",
            "Epoch [2/10], Loss: 1.0921, Validation Accuracy: 31.73%\n",
            "Epoch [3/10], Loss: 1.0622, Validation Accuracy: 32.27%\n",
            "Epoch [4/10], Loss: 1.1488, Validation Accuracy: 32.40%\n",
            "Epoch [5/10], Loss: 1.0908, Validation Accuracy: 31.20%\n",
            "Epoch [6/10], Loss: 0.9737, Validation Accuracy: 32.93%\n",
            "Epoch [7/10], Loss: 0.9888, Validation Accuracy: 31.33%\n",
            "Epoch [8/10], Loss: 0.9579, Validation Accuracy: 33.47%\n",
            "Epoch [9/10], Loss: 0.9435, Validation Accuracy: 32.67%\n",
            "Epoch [10/10], Loss: 1.1682, Validation Accuracy: 33.87%\n",
            "Test Accuracy: 33.47%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Parameters for synthetic data\n",
        "num_patients = 5000             # Total number of samples (patients)\n",
        "max_sequence_length = 150       # Maximum length of time series per patient\n",
        "num_sensors = 2                 # Number of sensors\n",
        "num_measurements_per_sensor = 5 # Measurements per sensor per timestamp\n",
        "num_static_features = 3         # Static features (e.g., age, height, gender)\n",
        "num_classes = 3                 # Number of classes for classification\n",
        "\n",
        "# New temporal input size\n",
        "input_size = num_sensors * num_measurements_per_sensor  # 2 * 5 = 10\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Generate random synthetic data with varying sequence lengths\n",
        "class VariableLengthDataset(Dataset):\n",
        "    def __init__(self, num_patients, max_sequence_length, input_size, num_static_features, num_classes):\n",
        "        self.num_patients = num_patients\n",
        "        self.input_size = input_size\n",
        "        self.num_static_features = num_static_features\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Generate synthetic data\n",
        "        self.temporal_data = []\n",
        "        self.sequence_lengths = []\n",
        "        self.static_data = torch.randn(num_patients, num_static_features)  # Static features\n",
        "        self.targets = torch.randint(0, num_classes, (num_patients,))      # Class labels\n",
        "\n",
        "        for _ in range(num_patients):\n",
        "            seq_len = np.random.randint(1, max_sequence_length + 1)\n",
        "            self.sequence_lengths.append(seq_len)\n",
        "            temporal_seq = torch.randn(seq_len, input_size)\n",
        "            self.temporal_data.append(temporal_seq)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_patients\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.temporal_data[idx], self.static_data[idx], self.targets[idx], self.sequence_lengths[idx]\n",
        "\n",
        "# Instantiate dataset and split it\n",
        "dataset = VariableLengthDataset(num_patients, max_sequence_length, input_size, num_static_features, num_classes)\n",
        "train_size = int(0.7 * num_patients)\n",
        "val_size = int(0.15 * num_patients)\n",
        "test_size = num_patients - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Custom collate function for padding sequences\n",
        "def collate_fn(batch):\n",
        "    temporal_data, static_data, targets, seq_lengths = zip(*batch)\n",
        "    seq_lengths = torch.tensor(seq_lengths)\n",
        "    static_data = torch.stack(static_data)\n",
        "    targets = torch.tensor(targets)\n",
        "\n",
        "    # Pad temporal data sequences to match the longest sequence in the batch\n",
        "    temporal_data_padded = pad_sequence(temporal_data, batch_first=True)\n",
        "\n",
        "    return temporal_data_padded, static_data, targets, seq_lengths\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define the Deep State Space Model for Classification with variable sequence lengths\n",
        "class DeepStateSpaceModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, static_input_size, num_classes, num_layers=1):\n",
        "        super(DeepStateSpaceModel, self).__init__()\n",
        "\n",
        "        # LSTM for temporal data\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Linear layer for static data\n",
        "        self.static_fc = nn.Linear(static_input_size, hidden_size)\n",
        "\n",
        "        # Combining LSTM output and static features\n",
        "        self.fc_combined = nn.Linear(hidden_size * 2, hidden_size)\n",
        "\n",
        "        # Output layer (for classification, matches num_classes)\n",
        "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, temporal_data, static_data, seq_lengths):\n",
        "        # Pack and pass temporal data through LSTM\n",
        "        packed_input = pack_padded_sequence(temporal_data, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, _) = self.lstm(packed_input)\n",
        "\n",
        "        # Get the final hidden state for the last element in each sequence\n",
        "        lstm_out = hn[-1, :, :]\n",
        "\n",
        "        # Pass static data through a linear layer\n",
        "        static_out = self.static_fc(static_data)\n",
        "\n",
        "        # Concatenate LSTM output with static output\n",
        "        combined = torch.cat((lstm_out, static_out), dim=1)\n",
        "\n",
        "        # Pass through a fully connected layer to combine features\n",
        "        combined_out = torch.relu(self.fc_combined(combined))\n",
        "\n",
        "        # Final output layer with no activation (use CrossEntropyLoss which applies softmax)\n",
        "        output = self.output_layer(combined_out)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = DeepStateSpaceModel(input_size=input_size, hidden_size=hidden_size,\n",
        "                            static_input_size=num_static_features, num_classes=num_classes, num_layers=num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # For classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with validation\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch in train_loader:\n",
        "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(temporal_batch, static_batch, seq_lengths)\n",
        "        loss = criterion(output, target_batch)  # CrossEntropyLoss expects raw logits\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop to calculate accuracy on validation data\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "            output = model(temporal_batch, static_batch, seq_lengths)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(output, 1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Final test accuracy calculation\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "        output = model(temporal_batch, static_batch, seq_lengths)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(output, 1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93794852",
      "metadata": {
        "id": "93794852"
      },
      "source": [
        "## Deep State Model combining above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e914b88c",
      "metadata": {
        "id": "e914b88c",
        "outputId": "8758e088-9f98-485d-89ab-07aa2583c17f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.1392, Validation Accuracy: 34.93%\n",
            "Epoch [2/10], Loss: 1.0977, Validation Accuracy: 34.67%\n",
            "Epoch [3/10], Loss: 1.1168, Validation Accuracy: 33.73%\n",
            "Epoch [4/10], Loss: 1.1242, Validation Accuracy: 32.00%\n",
            "Epoch [5/10], Loss: 1.0711, Validation Accuracy: 32.27%\n",
            "Epoch [6/10], Loss: 1.1099, Validation Accuracy: 33.07%\n",
            "Epoch [7/10], Loss: 1.0059, Validation Accuracy: 33.60%\n",
            "Epoch [8/10], Loss: 1.1149, Validation Accuracy: 32.67%\n",
            "Epoch [9/10], Loss: 1.0510, Validation Accuracy: 32.13%\n",
            "Epoch [10/10], Loss: 0.8642, Validation Accuracy: 31.60%\n",
            "Test Accuracy: 32.93%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Parameters for synthetic data\n",
        "num_patients = 5000             # Total number of samples (patients)\n",
        "max_sequence_length = 150       # Maximum length of time series per patient\n",
        "num_sensors = 2                 # Number of sensors\n",
        "num_measurements_per_sensor = 5 # Measurements per sensor per timestamp\n",
        "num_static_features = 3         # Static features (e.g., age, height, gender)\n",
        "num_classes = 3                 # Number of classes for classification\n",
        "\n",
        "# New temporal input size\n",
        "input_size = num_sensors * num_measurements_per_sensor  # 2 * 5 = 10\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Generate random synthetic data with varying sequence lengths\n",
        "class VariableLengthDataset(Dataset):\n",
        "    def __init__(self, num_patients, max_sequence_length, input_size, num_static_features, num_classes):\n",
        "        self.num_patients = num_patients\n",
        "        self.input_size = input_size\n",
        "        self.num_static_features = num_static_features\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Generate synthetic data\n",
        "        self.temporal_data = []\n",
        "        self.sequence_lengths = []\n",
        "        self.static_data = torch.randn(num_patients, num_static_features)  # Static features\n",
        "        self.targets = torch.randint(0, num_classes, (num_patients,))      # Class labels\n",
        "\n",
        "        for _ in range(num_patients):\n",
        "            seq_len = np.random.randint(1, max_sequence_length + 1)\n",
        "            self.sequence_lengths.append(seq_len)\n",
        "            temporal_seq = torch.randn(seq_len, input_size)\n",
        "            self.temporal_data.append(temporal_seq)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_patients\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.temporal_data[idx], self.static_data[idx], self.targets[idx], self.sequence_lengths[idx]\n",
        "\n",
        "# Instantiate dataset and split it\n",
        "dataset = VariableLengthDataset(num_patients, max_sequence_length, input_size, num_static_features, num_classes)\n",
        "train_size = int(0.7 * num_patients)\n",
        "val_size = int(0.15 * num_patients)\n",
        "test_size = num_patients - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Custom collate function for padding sequences\n",
        "def collate_fn(batch):\n",
        "    temporal_data, static_data, targets, seq_lengths = zip(*batch)\n",
        "    seq_lengths = torch.tensor(seq_lengths)\n",
        "    static_data = torch.stack(static_data)\n",
        "    targets = torch.tensor(targets)\n",
        "\n",
        "    # Pad temporal data sequences to match the longest sequence in the batch\n",
        "    temporal_data_padded = pad_sequence(temporal_data, batch_first=True)\n",
        "\n",
        "    return temporal_data_padded, static_data, targets, seq_lengths\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define a Deep State Space Model (DSSM) for classification\n",
        "class DeepStateSpaceModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, static_input_size, num_classes, num_layers=1):\n",
        "        super(DeepStateSpaceModel, self).__init__()\n",
        "\n",
        "        # LSTM for temporal data (this is the \"observation model\")\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Linear layer for static data (this is the \"static feature processing\")\n",
        "        self.static_fc = nn.Linear(static_input_size, hidden_size)\n",
        "\n",
        "        # State transition model (latent state evolution)\n",
        "        self.transition_matrix = nn.Parameter(torch.randn(hidden_size, hidden_size))  # Linear state transition matrix\n",
        "\n",
        "        # Output layer (for classification)\n",
        "        self.output_layer = nn.Linear(hidden_size * 2, num_classes)  # Changed from hidden_size to hidden_size * 2\n",
        "\n",
        "    def forward(self, temporal_data, static_data, seq_lengths):\n",
        "        # Pack and pass temporal data through LSTM\n",
        "        packed_input = pack_padded_sequence(temporal_data, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, _) = self.lstm(packed_input)\n",
        "\n",
        "        # Get the final hidden state (latent state at the last time step)\n",
        "        lstm_out = hn[-1, :, :]  # shape: [batch_size, hidden_size]\n",
        "\n",
        "        # Apply transition model (latent state dynamics)\n",
        "        lstm_out = torch.matmul(lstm_out, self.transition_matrix)  # shape: [batch_size, hidden_size]\n",
        "\n",
        "        # Pass static data through a linear layer\n",
        "        static_out = self.static_fc(static_data)  # shape: [batch_size, hidden_size]\n",
        "\n",
        "        # Combine the LSTM output with static features\n",
        "        combined = torch.cat((lstm_out, static_out), dim=1)  # shape: [batch_size, hidden_size * 2]\n",
        "\n",
        "        # Output classification\n",
        "        output = self.output_layer(combined)  # shape: [batch_size, num_classes]\n",
        "\n",
        "        return output\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = DeepStateSpaceModel(input_size=input_size, hidden_size=hidden_size,\n",
        "                            static_input_size=num_static_features, num_classes=num_classes, num_layers=num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # For classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop with validation\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch in train_loader:\n",
        "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(temporal_batch, static_batch, seq_lengths)\n",
        "        loss = criterion(output, target_batch)  # CrossEntropyLoss expects raw logits\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop to calculate accuracy on validation data\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "            output = model(temporal_batch, static_batch, seq_lengths)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(output, 1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Final test accuracy calculation\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "        output = model(temporal_batch, static_batch, seq_lengths)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(output, 1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc6abc62",
      "metadata": {
        "id": "dc6abc62"
      },
      "source": [
        "## Final DSSM: Adding State Space Evolution (representing the evolution of the hidden state over time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "031f2c5e",
      "metadata": {
        "id": "031f2c5e",
        "outputId": "0187be13-84f8-413f-b610-641592691e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.0879, Validation Accuracy: 34.27%\n",
            "Epoch [2/10], Loss: 1.1007, Validation Accuracy: 32.13%\n",
            "Epoch [3/10], Loss: 1.1178, Validation Accuracy: 31.20%\n",
            "Epoch [4/10], Loss: 1.0971, Validation Accuracy: 33.60%\n",
            "Epoch [5/10], Loss: 1.0784, Validation Accuracy: 33.33%\n",
            "Epoch [6/10], Loss: 1.1164, Validation Accuracy: 36.00%\n",
            "Epoch [7/10], Loss: 1.1089, Validation Accuracy: 35.60%\n",
            "Epoch [8/10], Loss: 1.1082, Validation Accuracy: 33.87%\n",
            "Epoch [9/10], Loss: 1.0577, Validation Accuracy: 34.93%\n",
            "Epoch [10/10], Loss: 1.1136, Validation Accuracy: 32.80%\n",
            "Test Accuracy: 33.07%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "import numpy as np\n",
        "\n",
        "# Parameters for synthetic data\n",
        "num_patients = 5000             # Total number of samples (patients)\n",
        "max_sequence_length = 150       # Maximum length of time series per patient\n",
        "num_sensors = 2                 # Number of sensors\n",
        "num_measurements_per_sensor = 5 # Measurements per sensor per timestamp\n",
        "num_static_features = 3         # Static features (e.g., age, height, gender)\n",
        "num_classes = 3                 # Number of classes for classification\n",
        "\n",
        "# New temporal input size\n",
        "input_size = num_sensors * num_measurements_per_sensor  # 2 * 5 = 10\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Generate random synthetic data with varying sequence lengths\n",
        "class VariableLengthDataset(Dataset):\n",
        "    def __init__(self, num_patients, max_sequence_length, input_size, num_static_features, num_classes):\n",
        "        self.num_patients = num_patients\n",
        "        self.input_size = input_size\n",
        "        self.num_static_features = num_static_features\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Generate synthetic data\n",
        "        self.temporal_data = []\n",
        "        self.sequence_lengths = []\n",
        "        self.static_data = torch.randn(num_patients, num_static_features)  # Static features\n",
        "        self.targets = torch.randint(0, num_classes, (num_patients,))      # Class labels\n",
        "\n",
        "        for _ in range(num_patients):\n",
        "            seq_len = np.random.randint(1, max_sequence_length + 1)\n",
        "            self.sequence_lengths.append(seq_len)\n",
        "            temporal_seq = torch.randn(seq_len, input_size)\n",
        "            self.temporal_data.append(temporal_seq)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_patients\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.temporal_data[idx], self.static_data[idx], self.targets[idx], self.sequence_lengths[idx]\n",
        "\n",
        "# Instantiate dataset and split it\n",
        "dataset = VariableLengthDataset(num_patients, max_sequence_length, input_size, num_static_features, num_classes)\n",
        "train_size = int(0.7 * num_patients)\n",
        "val_size = int(0.15 * num_patients)\n",
        "test_size = num_patients - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Custom collate function for padding sequences\n",
        "def collate_fn(batch):\n",
        "    temporal_data, static_data, targets, seq_lengths = zip(*batch)\n",
        "    seq_lengths = torch.tensor(seq_lengths)\n",
        "    static_data = torch.stack(static_data)\n",
        "    targets = torch.tensor(targets)\n",
        "\n",
        "    # Pad temporal data sequences to match the longest sequence in the batch\n",
        "    temporal_data_padded = pad_sequence(temporal_data, batch_first=True)\n",
        "\n",
        "    return temporal_data_padded, static_data, targets, seq_lengths\n",
        "\n",
        "# Create DataLoaders for batching\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Define a Deep State Space Model (DSSM) for classification\n",
        "class DeepStateSpaceModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, static_input_size, num_classes, num_layers=1):\n",
        "        super(DeepStateSpaceModel, self).__init__()\n",
        "\n",
        "        # LSTM for temporal data (this is the \"observation model\")\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Linear layer for static data (this is the \"static feature processing\")\n",
        "        self.static_fc = nn.Linear(static_input_size, hidden_size)\n",
        "\n",
        "        # Transition matrix for the state space model (latent state evolution)\n",
        "        self.transition_matrix = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)  # Scaled initialization\n",
        "\n",
        "        # Output layer (for classification)\n",
        "        self.output_layer = nn.Linear(hidden_size * 2, num_classes)  # Changed from hidden_size to hidden_size * 2\n",
        "\n",
        "    def forward(self, temporal_data, static_data, seq_lengths):\n",
        "        # Pack and pass temporal data through LSTM\n",
        "        packed_input = pack_padded_sequence(temporal_data, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hn, _) = self.lstm(packed_input)\n",
        "\n",
        "        # Initial latent state (from LSTM's final hidden state)\n",
        "        latent_state = hn[-1, :, :]  # shape: [batch_size, hidden_size]\n",
        "\n",
        "        # State space evolution: evolve the latent state over time using the transition matrix\n",
        "        for t in range(1, temporal_data.size(1)):  # Loop through time steps (sequence length)\n",
        "            latent_state = torch.matmul(latent_state, self.transition_matrix)  # Transition from previous state to the next\n",
        "\n",
        "        # Pass static data through a linear layer\n",
        "        static_out = self.static_fc(static_data)  # shape: [batch_size, hidden_size]\n",
        "\n",
        "        # Combine the LSTM output with static features\n",
        "        combined = torch.cat((latent_state, static_out), dim=1)  # shape: [batch_size, hidden_size * 2]\n",
        "\n",
        "        # Output classification\n",
        "        output = self.output_layer(combined)  # shape: [batch_size, num_classes]\n",
        "\n",
        "        return output\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = DeepStateSpaceModel(input_size=input_size, hidden_size=hidden_size,\n",
        "                            static_input_size=num_static_features, num_classes=num_classes, num_layers=num_layers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # For classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Gradient clipping to avoid exploding gradients\n",
        "max_grad_norm = 1.0  # Clip gradients to a maximum norm of 1.0\n",
        "\n",
        "# Training loop with validation\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch in train_loader:\n",
        "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(temporal_batch, static_batch, seq_lengths)\n",
        "        loss = criterion(output, target_batch)  # CrossEntropyLoss expects raw logits\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop to calculate accuracy on validation data\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "            output = model(temporal_batch, static_batch, seq_lengths)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(output, 1)\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Final test accuracy calculation\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
        "        output = model(temporal_batch, static_batch, seq_lengths)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(output, 1)\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f33360",
      "metadata": {
        "id": "c3f33360"
      },
      "source": [
        "Components of the Model:\n",
        "\n",
        "LSTM Layer: This processes the variable-length time-series data. The LSTM takes the temporal input and extracts features over time. The LSTM's final hidden state (hn[-1]) represents the learned latent state of the sequence. The LSTM is designed to handle sequences of varying lengths, which is done using the pack_padded_sequence function to efficiently process padded sequences.\n",
        "\n",
        "Transition Matrix: After obtaining the final LSTM hidden state, the model introduces a state-space model with a transition matrix. This matrix is used to evolve the latent state over time. For each time step (from 1 to the sequence length), the latent state is updated by multiplying the previous latent state by this transition matrix. This mechanism helps the model learn how the latent state evolves over time, potentially capturing dynamics in the patient's time-series data.\n",
        "\n",
        "Static Feature Processing: The static features (e.g., age, gender) are passed through a fully connected layer (static_fc), which transforms them into a hidden space of the same dimension as the LSTM output. This allows the static features to be integrated with the temporal sequence in a consistent latent space.\n",
        "\n",
        "Combining Temporal and Static Features: The final representation is obtained by concatenating the updated latent state (from the state-space model) with the processed static features.\n",
        "\n",
        "Output Layer: A fully connected output layer (output_layer) produces the final prediction, which is a vector of class logits (size = number of classes). The logits are passed through a softmax function (implicitly inside the loss function) during training to get probabilities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}