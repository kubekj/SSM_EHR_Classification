{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d01602",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def load_data(file_path):\n",
    "  data = np.load(\"./P12data/split_1/\" + file_path, allow_pickle=True)\n",
    "\n",
    "  # Let's create lists to hold the extracted data\n",
    "  ts_values_list = []\n",
    "  ts_indicators_list = []\n",
    "  ts_times_list = []\n",
    "  static_features_list = []\n",
    "  labels_list = []\n",
    "\n",
    "  # Iterate through all records to collect the data\n",
    "  for record in data:\n",
    "      # Extract the timeseries values, indicators, and timestamps\n",
    "      ts_values = record['ts_values']\n",
    "      ts_indicators = record['ts_indicators']\n",
    "      ts_times = record['ts_times']\n",
    "\n",
    "      # Flatten the ts_values if it's a 2D array, or leave it as is if 1D\n",
    "      #ts_values_flat = ts_values.flatten() if ts_values.ndim > 1 else ts_values\n",
    "\n",
    "      # Extract the static features and labels (assuming they're 1D arrays)\n",
    "      static_features = record['static']\n",
    "      labels = record['labels']\n",
    "\n",
    "      # Append the extracted values to the respective lists\n",
    "      ts_values_list.append(ts_values)\n",
    "      ts_indicators_list.append(ts_indicators)\n",
    "      ts_times_list.append(ts_times)\n",
    "      static_features_list.append(static_features)\n",
    "      labels_list.append(labels)\n",
    "\n",
    "  # Create a DataFrame from the collected lists\n",
    "  df = pd.DataFrame({\n",
    "      'ts_values': ts_values_list,\n",
    "      'ts_indicators': ts_indicators_list,\n",
    "      'ts_times': ts_times_list,\n",
    "      'static': static_features_list,\n",
    "      'labels': labels_list\n",
    "  })\n",
    "  df['ts_indicators'] = df['ts_indicators'].apply(lambda x: np.array(x, dtype=int))\n",
    "  df['ts_indicators']\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c0d10c",
   "metadata": {
    "id": "12c0d10c",
    "outputId": "f7e0c147-2677-4a48-f81a-d3c2347f1f22"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'P12data/split_1/train_physionet2012_1.npy'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load the data from the .npy file\u001B[39;00m\n\u001B[0;32m      2\u001B[0m file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_physionet2012_1.npy\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 3\u001B[0m df_train \u001B[38;5;241m=\u001B[39m load_data(file_path)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Display the first few rows of the DataFrame\u001B[39;00m\n\u001B[0;32m      5\u001B[0m df_train\u001B[38;5;241m.\u001B[39mhead()\n",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m, in \u001B[0;36mload_data\u001B[1;34m(file_path)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data\u001B[39m(file_path):\n\u001B[1;32m----> 4\u001B[0m   data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP12data/split_1/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m file_path, allow_pickle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      6\u001B[0m   \u001B[38;5;66;03m# Let's create lists to hold the extracted data\u001B[39;00m\n\u001B[0;32m      7\u001B[0m   ts_values_list \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32mc:\\ProgramData\\anaconda3\\envs\\ML\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    425\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    426\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 427\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28mopen\u001B[39m(os_fspath(file), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m    428\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    430\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'P12data/split_1/train_physionet2012_1.npy'"
     ]
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "file_path = '../P12data/split_1/train_physionet2012_1.npy'\n",
    "df_train = load_data(file_path)\n",
    "# Display the first few rows of the DataFrame\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b3e95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_values</th>\n",
       "      <th>ts_indicators</th>\n",
       "      <th>ts_times</th>\n",
       "      <th>static</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.16966099858624017, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 1.0166667, 1.9166666, 2.95, 3.7, 3.78333...</td>\n",
       "      <td>[0.3201678628319356, 0.0, 1.0, 1.0059237867031...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.32848355450987454, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.15, 0.65, 1.65, 2.65, 3.15, 3.4, 4.15,...</td>\n",
       "      <td>[0.5540780043938743, 0.0, 1.0, 1.0059237867031...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.9359767612187836, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.31666666, 0.85, 1.25, 2.1, 2.35, 2.6, ...</td>\n",
       "      <td>[-0.2646074910729112, 1.0, 0.0, -1.04133971147...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.9481265944542595, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.15, 0.23333333, 0.4, 0.65, 1.15, 1.65,...</td>\n",
       "      <td>[-1.2587255927111507, 0.0, 1.0, -1.04133971147...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.5755307301074263, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.1, 1.1, 2.1, 2.15, 3.1, 4.1, 5.1, 6.1,...</td>\n",
       "      <td>[-0.03069734951097247, 1.0, 0.0, 0.97573151501...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ts_values  \\\n",
       "0  [[0.16966099858624017, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "1  [[-0.32848355450987454, 0.0, 0.0, 0.0, 0.0, 0....   \n",
       "2  [[-0.9359767612187836, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "3  [[-0.9481265944542595, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "4  [[-0.5755307301074263, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                       ts_indicators  \\\n",
       "0  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                            ts_times  \\\n",
       "0  [0.0, 1.0166667, 1.9166666, 2.95, 3.7, 3.78333...   \n",
       "1  [0.0, 0.15, 0.65, 1.65, 2.65, 3.15, 3.4, 4.15,...   \n",
       "2  [0.0, 0.31666666, 0.85, 1.25, 2.1, 2.35, 2.6, ...   \n",
       "3  [0.0, 0.15, 0.23333333, 0.4, 0.65, 1.15, 1.65,...   \n",
       "4  [0.0, 0.1, 1.1, 2.1, 2.15, 3.1, 4.1, 5.1, 6.1,...   \n",
       "\n",
       "                                              static  labels  \n",
       "0  [0.3201678628319356, 0.0, 1.0, 1.0059237867031...       0  \n",
       "1  [0.5540780043938743, 0.0, 1.0, 1.0059237867031...       0  \n",
       "2  [-0.2646074910729112, 1.0, 0.0, -1.04133971147...       0  \n",
       "3  [-1.2587255927111507, 0.0, 1.0, -1.04133971147...       0  \n",
       "4  [-0.03069734951097247, 1.0, 0.0, 0.97573151501...       0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "file_path = '../P12data/split_1/test_physionet2012_1.npy'\n",
    "df_test = load_data(file_path)\n",
    "# Display the first few rows of the DataFrame\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77319c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_values</th>\n",
       "      <th>ts_indicators</th>\n",
       "      <th>ts_times</th>\n",
       "      <th>static</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.6079304913933774, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.11666667, 0.85, 0.93333334, 1.0166667,...</td>\n",
       "      <td>[-0.2061299556824265, 0.0, 1.0, 1.005923786703...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-1.2599731381302017, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.4, 0.65, 0.9, 1.4, 1.7833333, 1.9, 2.3...</td>\n",
       "      <td>[-0.6154727034158193, 1.0, 0.0, 0.887477495879...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.2104992358193618, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.2, 0.26666668, 0.51666665, 0.76666665,...</td>\n",
       "      <td>[0.43712293361290494, 1.0, 0.0, 0.740000065545...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-3.406449135168347, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.05, 0.3, 0.55, 1.55, 2.55, 3.55, 4.55,...</td>\n",
       "      <td>[-0.9663379157587273, 1.0, 0.0, -1.04133971147...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.6686798120642683, 0.0, 0.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.05, 0.55, 1.05, 1.0666667, 1.3, 2.05, ...</td>\n",
       "      <td>[1.2558084290796905, 0.0, 1.0, 0.9467005526959...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           ts_values  \\\n",
       "0  [[-0.6079304913933774, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "1  [[-1.2599731381302017, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "2  [[1.2104992358193618, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[-3.406449135168347, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4  [[-0.6686798120642683, 0.0, 0.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                       ts_indicators  \\\n",
       "0  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "1  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "4  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                            ts_times  \\\n",
       "0  [0.0, 0.11666667, 0.85, 0.93333334, 1.0166667,...   \n",
       "1  [0.0, 0.4, 0.65, 0.9, 1.4, 1.7833333, 1.9, 2.3...   \n",
       "2  [0.0, 0.2, 0.26666668, 0.51666665, 0.76666665,...   \n",
       "3  [0.0, 0.05, 0.3, 0.55, 1.55, 2.55, 3.55, 4.55,...   \n",
       "4  [0.0, 0.05, 0.55, 1.05, 1.0666667, 1.3, 2.05, ...   \n",
       "\n",
       "                                              static  labels  \n",
       "0  [-0.2061299556824265, 0.0, 1.0, 1.005923786703...       0  \n",
       "1  [-0.6154727034158193, 1.0, 0.0, 0.887477495879...       0  \n",
       "2  [0.43712293361290494, 1.0, 0.0, 0.740000065545...       0  \n",
       "3  [-0.9663379157587273, 1.0, 0.0, -1.04133971147...       0  \n",
       "4  [1.2558084290796905, 0.0, 1.0, 0.9467005526959...       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the .npy file\n",
    "file_path = '../P12data/split_1/validation_physionet2012_1.npy'\n",
    "df_val = load_data(file_path)\n",
    "# Display the first few rows of the DataFrame\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315efae5",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8f5c4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "0    0.600029\n",
      "1    0.399971\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df_train[df_train.labels == 0]\n",
    "df_minority = df_train[df_train.labels == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                 replace=True,     # sample with replacement\n",
    "                 n_samples=int(len(df_majority)*40/60),    # to match majority class\n",
    "                 random_state=123) # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_train_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Display new class counts\n",
    "print(df_train_upsampled.labels.value_counts()/len(df_train_upsampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "265ebf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa6414",
   "metadata": {},
   "source": [
    "# Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e9320ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_saved = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f32eefe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train_saved.sample(frac=0.05).reset_index(drop=True)\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27cd52a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3af18d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters based on your data\n",
    "max_sequence_length_train = df_train['ts_times'].apply(len).max()  # Max sequence length based on ts_times\n",
    "max_sequence_length_test = df_test['ts_times'].apply(len).max()  # Max sequence length based on ts_times\n",
    "max_sequence_length_val = df_val['ts_times'].apply(len).max()\n",
    "\n",
    "num_sensors = 2                    # Number of sensors (now 2)\n",
    "num_measurements_per_sensor = 37   # Measurements per sensor per timestamp\n",
    "num_static_features = 8            # Static features (age, height, etc.)\n",
    "num_classes = 2                    # Number of classes (0 or 1)\n",
    "\n",
    "# New temporal input size (2 sensors with 37 measurements each)\n",
    "input_size = num_sensors * num_measurements_per_sensor  # 2 * 37 = 74\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b3d62c8",
   "metadata": {
    "id": "0b3d62c8",
    "outputId": "d6180a18-f6c3-47a2-8d9d-58cf60698360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.2484, Validation Accuracy: 43.81%\n",
      "Epoch [2/100], Loss: 0.8381, Validation Accuracy: 47.74%\n",
      "Epoch [3/100], Loss: 0.4558, Validation Accuracy: 49.05%\n",
      "Epoch [4/100], Loss: 0.6637, Validation Accuracy: 52.84%\n",
      "Epoch [5/100], Loss: 0.6905, Validation Accuracy: 53.42%\n",
      "Epoch [6/100], Loss: 0.5394, Validation Accuracy: 46.00%\n",
      "Epoch [7/100], Loss: 0.3549, Validation Accuracy: 49.78%\n",
      "Epoch [8/100], Loss: 0.2635, Validation Accuracy: 49.64%\n",
      "Epoch [9/100], Loss: 0.4168, Validation Accuracy: 46.58%\n",
      "Epoch [10/100], Loss: 0.4445, Validation Accuracy: 52.69%\n",
      "Epoch [11/100], Loss: 0.4553, Validation Accuracy: 48.91%\n",
      "Epoch [12/100], Loss: 0.5088, Validation Accuracy: 55.02%\n",
      "Epoch [13/100], Loss: 0.4359, Validation Accuracy: 51.53%\n",
      "Epoch [14/100], Loss: 0.5852, Validation Accuracy: 55.31%\n",
      "Epoch [15/100], Loss: 0.5514, Validation Accuracy: 56.48%\n",
      "Epoch [16/100], Loss: 0.4658, Validation Accuracy: 47.74%\n",
      "Epoch [17/100], Loss: 0.5726, Validation Accuracy: 49.20%\n",
      "Epoch [18/100], Loss: 0.7176, Validation Accuracy: 45.71%\n",
      "Epoch [19/100], Loss: 0.6417, Validation Accuracy: 55.75%\n",
      "Epoch [20/100], Loss: 0.7809, Validation Accuracy: 50.07%\n",
      "Epoch [21/100], Loss: 0.2885, Validation Accuracy: 53.57%\n",
      "Epoch [22/100], Loss: 0.4869, Validation Accuracy: 49.20%\n",
      "Epoch [23/100], Loss: 0.3578, Validation Accuracy: 50.66%\n",
      "Epoch [24/100], Loss: 1.1653, Validation Accuracy: 53.42%\n",
      "Epoch [25/100], Loss: 0.5860, Validation Accuracy: 53.71%\n",
      "Epoch [26/100], Loss: 0.4490, Validation Accuracy: 52.11%\n",
      "Epoch [27/100], Loss: 0.4685, Validation Accuracy: 56.04%\n",
      "Epoch [28/100], Loss: 0.6129, Validation Accuracy: 57.64%\n",
      "Epoch [29/100], Loss: 0.4297, Validation Accuracy: 55.17%\n",
      "Epoch [30/100], Loss: 0.5555, Validation Accuracy: 50.22%\n",
      "Epoch [31/100], Loss: 0.2598, Validation Accuracy: 55.75%\n",
      "Epoch [32/100], Loss: 0.8333, Validation Accuracy: 58.66%\n",
      "Epoch [33/100], Loss: 0.2923, Validation Accuracy: 55.02%\n",
      "Epoch [34/100], Loss: 0.3623, Validation Accuracy: 55.60%\n",
      "Epoch [35/100], Loss: 0.3667, Validation Accuracy: 54.00%\n",
      "Epoch [36/100], Loss: 0.5322, Validation Accuracy: 55.60%\n",
      "Epoch [37/100], Loss: 0.3621, Validation Accuracy: 53.13%\n",
      "Epoch [38/100], Loss: 0.4284, Validation Accuracy: 53.71%\n",
      "Epoch [39/100], Loss: 0.3762, Validation Accuracy: 50.22%\n",
      "Epoch [40/100], Loss: 0.7338, Validation Accuracy: 54.59%\n",
      "Epoch [41/100], Loss: 0.4826, Validation Accuracy: 58.81%\n",
      "Epoch [42/100], Loss: 0.5149, Validation Accuracy: 55.31%\n",
      "Epoch [43/100], Loss: 0.6991, Validation Accuracy: 54.15%\n",
      "Epoch [44/100], Loss: 0.2901, Validation Accuracy: 53.28%\n",
      "Epoch [45/100], Loss: 0.5381, Validation Accuracy: 58.37%\n",
      "Epoch [46/100], Loss: 0.3613, Validation Accuracy: 50.66%\n",
      "Epoch [47/100], Loss: 0.3266, Validation Accuracy: 54.00%\n",
      "Epoch [48/100], Loss: 0.2914, Validation Accuracy: 55.90%\n",
      "Epoch [49/100], Loss: 0.3616, Validation Accuracy: 53.42%\n",
      "Epoch [50/100], Loss: 0.2024, Validation Accuracy: 57.64%\n",
      "Epoch [51/100], Loss: 0.4621, Validation Accuracy: 56.91%\n",
      "Epoch [52/100], Loss: 1.1420, Validation Accuracy: 54.29%\n",
      "Epoch [53/100], Loss: 0.5421, Validation Accuracy: 54.00%\n",
      "Epoch [54/100], Loss: 0.6824, Validation Accuracy: 52.69%\n",
      "Epoch [55/100], Loss: 0.3373, Validation Accuracy: 61.14%\n",
      "Epoch [56/100], Loss: 0.8849, Validation Accuracy: 58.81%\n",
      "Epoch [57/100], Loss: 0.2291, Validation Accuracy: 53.86%\n",
      "Epoch [58/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [59/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [60/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [61/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [62/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [63/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [64/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [65/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [66/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [67/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [68/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [69/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [70/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [71/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [72/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [73/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [74/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [75/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [76/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [77/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [78/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [79/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [80/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [81/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [82/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [83/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [84/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [85/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [86/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [87/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [88/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [89/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [90/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [91/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [92/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [93/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [94/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [95/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [96/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [97/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [98/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [99/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Epoch [100/100], Loss: nan, Validation Accuracy: 60.55%\n",
      "Test Accuracy: 60.55%\n"
     ]
    }
   ],
   "source": [
    "# Adjusted dataset class for your data\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, df, max_sequence_length, input_size, num_static_features, num_classes):\n",
    "        self.df = df\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.input_size = input_size\n",
    "        self.num_static_features = num_static_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.temporal_data = []\n",
    "        self.static_data = []\n",
    "        self.targets = []\n",
    "        self.sequence_lengths = []\n",
    "\n",
    "        # Extract data\n",
    "        for idx, row in df.iterrows():\n",
    "            ts_values = torch.tensor(row['ts_values'], dtype=torch.float32, device=torch.cuda.current_device())  # Temporal data (sensor 1)\n",
    "            ts_indicators = torch.tensor(row['ts_indicators'], dtype=torch.float32, device=torch.cuda.current_device())  # Temporal data (sensor 2)\n",
    "            ts_times = torch.tensor(row['ts_times'], dtype=torch.float32, device=torch.cuda.current_device())  # Time spans (timestamps)\n",
    "            static_features = torch.tensor(row['static'], dtype=torch.float32, device=torch.cuda.current_device())  # Static features\n",
    "            label = row['labels']  # Class label\n",
    "\n",
    "            seq_len = len(ts_times)  # Sequence length\n",
    "            self.sequence_lengths.append(seq_len)\n",
    "\n",
    "            # Concatenate ts_values and ts_indicators to create a combined input for both sensors\n",
    "            combined_temporal_data = torch.cat((ts_values, ts_indicators), dim=-1)  # shape: [seq_len, 74]\n",
    "            self.temporal_data.append(combined_temporal_data)\n",
    "            self.static_data.append(static_features)\n",
    "            self.targets.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.temporal_data[idx], self.static_data[idx], self.targets[idx], self.sequence_lengths[idx]\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_data = PatientDataset(df_train, max_sequence_length_train, input_size, num_static_features, num_classes)\n",
    "test_data = train_data\n",
    "val_data = train_data\n",
    "\n",
    "# Custom collate function to pad sequences\n",
    "def collate_fn(batch):\n",
    "    temporal_data, static_data, targets, seq_lengths = zip(*batch)\n",
    "    seq_lengths = torch.tensor(seq_lengths, device=torch.cuda.current_device())\n",
    "    static_data = torch.stack(static_data)\n",
    "    targets = torch.tensor(targets, device=torch.cuda.current_device())\n",
    "\n",
    "    # Pad temporal data sequences to match the longest sequence in the batch\n",
    "    temporal_data_padded = pad_sequence(temporal_data, batch_first=True)\n",
    "    return temporal_data_padded, static_data, targets, seq_lengths\n",
    "\n",
    "# Create DataLoader instances\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "# Define a Deep State Space Model (DSSM) for classification\n",
    "class DeepStateSpaceModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, static_input_size, num_classes, num_layers=1):\n",
    "        super(DeepStateSpaceModel, self).__init__()\n",
    "\n",
    "        # LSTM for temporal data (this is the \"observation model\")\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, device=torch.cuda.current_device())\n",
    "        \n",
    "        # Linear layer for static data (this is the \"static feature processing\")\n",
    "        self.static_fc = nn.Linear(static_input_size, hidden_size, device=torch.cuda.current_device())\n",
    "\n",
    "        # Transition matrix for the state space model (latent state evolution)\n",
    "        self.transition_matrix = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.1)  # Scaled initialization\n",
    "\n",
    "        # Fully connected layer for combined features\n",
    "        self.combined_fc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # Output layer (for classification)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes, device=torch.cuda.current_device())  # Changed from hidden_size to hidden_size * 2\n",
    "\n",
    "        self.first = True\n",
    "        # Normalization layers\n",
    "        self.norm_temporal = nn.LayerNorm(hidden_size)\n",
    "        self.norm_static = nn.LayerNorm(hidden_size)\n",
    "        self.norm_combined = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, temporal_data, static_data, seq_lengths):\n",
    "        # Pack and pass temporal data through LSTM\n",
    "        # temporal_data = self.norm_1(temporal_data)\n",
    "        packed_input = pack_padded_sequence(temporal_data, seq_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hn, _) = self.lstm(packed_input)\n",
    "\n",
    "        # Initial latent state (from LSTM's final hidden state)\n",
    "        latent_state = hn[-1, :, :]  # shape: [batch_size, hidden_size]\n",
    "\n",
    "        # Apply non-linear transformation and normalization to latent state\n",
    "        latent_state =  nn.functional.relu(self.norm_temporal(latent_state))\n",
    "\n",
    "        # State space evolution: evolve the latent state over time using the transition matrix\n",
    "        for t in range(1, temporal_data.size(1)):  # Loop through time steps (sequence length)\n",
    "            latent_state = torch.matmul(latent_state, self.transition_matrix)  # Transition from previous state to the next\n",
    "\n",
    "        # Process static data with a fully connected layer\n",
    "        static_out = nn.functional.relu(self.norm_static(self.static_fc(static_data)))\n",
    "\n",
    "        # Combine the LSTM output with static features\n",
    "        combined = torch.cat((latent_state, static_out), dim=1)  # shape: [batch_size, hidden_size * 2]\n",
    "        combined = nn.functional.relu(self.norm_combined(self.combined_fc(combined)))\n",
    "        # Output classification\n",
    "        # combined = self.norm_3(combined)\n",
    "        output = self.output_layer(combined)  # shape: [batch_size, num_classes]\n",
    "\n",
    "        return output\n",
    "\n",
    "# Class weights for handling class imbalance\n",
    "class_weights = torch.tensor([1.0, 5.0], device=torch.cuda.current_device())  # Adjust the weights based on your class imbalance\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  # For classification with class imbalance\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = DeepStateSpaceModel(input_size=input_size, hidden_size=hidden_size,\n",
    "                            static_input_size=num_static_features, num_classes=num_classes, num_layers=num_layers)\n",
    "model = model.to(torch.cuda.current_device())\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Gradient clipping to avoid exploding gradients\n",
    "max_grad_norm = 5.0  # Clip gradients to a maximum norm of 1.0\n",
    "\n",
    "# Training loop with validation\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch in train_loader:\n",
    "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(temporal_batch, static_batch, seq_lengths)\n",
    "\n",
    "        loss = criterion(output, target_batch)  # CrossEntropyLoss expects raw logits\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation loop to calculate accuracy on validation data\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
    "            output = model(temporal_batch, static_batch, seq_lengths)\n",
    "\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(output, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(target_batch.cpu().numpy())\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Final test accuracy calculation\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        temporal_batch, static_batch, target_batch, seq_lengths = batch\n",
    "        output = model(temporal_batch, static_batch, seq_lengths)\n",
    "\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(output, 1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(target_batch.cpu().numpy())\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32db4da",
   "metadata": {},
   "source": [
    "- No batchnorm:\n",
    "Epoch [1/10], Loss: 0.4598, Validation Accuracy: 40.32%\n",
    "Epoch [2/10], Loss: 0.9196, Validation Accuracy: 43.38%\n",
    "Epoch [3/10], Loss: 0.7145, Validation Accuracy: 43.23%\n",
    "Epoch [4/10], Loss: 0.7549, Validation Accuracy: 46.58%\n",
    "Epoch [5/10], Loss: 0.6851, Validation Accuracy: 47.31%\n",
    "Epoch [6/10], Loss: 0.6319, Validation Accuracy: 46.72%\n",
    "Epoch [7/10], Loss: 0.4454, Validation Accuracy: 49.78%\n",
    "Epoch [8/10], Loss: 0.4319, Validation Accuracy: 50.07%\n",
    "Epoch [9/10], Loss: 0.3745, Validation Accuracy: 49.78%\n",
    "Epoch [10/10], Loss: 0.4979, Validation Accuracy: 53.42%\n",
    "Test Accuracy: 53.42%\n",
    "- Batchnorm 1:\n",
    "Epoch [1/10], Loss: 0.8834, Validation Accuracy: 40.47%\n",
    "Epoch [2/10], Loss: 0.4536, Validation Accuracy: 44.25%\n",
    "Epoch [3/10], Loss: 0.5721, Validation Accuracy: 46.43%\n",
    "Epoch [4/10], Loss: 0.4230, Validation Accuracy: 48.76%\n",
    "Epoch [5/10], Loss: 0.8273, Validation Accuracy: 49.20%\n",
    "Epoch [6/10], Loss: 0.3996, Validation Accuracy: 50.07%\n",
    "Epoch [7/10], Loss: 0.4100, Validation Accuracy: 49.78%\n",
    "Epoch [8/10], Loss: 0.4178, Validation Accuracy: 50.66%\n",
    "Epoch [9/10], Loss: 0.6027, Validation Accuracy: 51.97%\n",
    "Epoch [10/10], Loss: 0.3193, Validation Accuracy: 50.51%\n",
    "Test Accuracy: 50.51%\n",
    "- Batchnorm 2:\n",
    "Epoch [1/10], Loss: 0.5966, Validation Accuracy: 51.82%\n",
    "Epoch [2/10], Loss: 0.3539, Validation Accuracy: 51.09%\n",
    "Epoch [3/10], Loss: 0.2553, Validation Accuracy: 48.76%\n",
    "Epoch [4/10], Loss: 0.3089, Validation Accuracy: 49.49%\n",
    "Epoch [5/10], Loss: 0.3994, Validation Accuracy: 51.67%\n",
    "Epoch [6/10], Loss: 0.3683, Validation Accuracy: 49.93%\n",
    "Epoch [7/10], Loss: 0.4936, Validation Accuracy: 50.07%\n",
    "Epoch [8/10], Loss: 0.5954, Validation Accuracy: 50.51%\n",
    "Epoch [9/10], Loss: 0.5542, Validation Accuracy: 50.36%\n",
    "Epoch [10/10], Loss: 0.5420, Validation Accuracy: 50.22%\n",
    "Test Accuracy: 50.22%\n",
    "- Batchnorm 3:\n",
    "Epoch [1/10], Loss: 0.6734, Validation Accuracy: 51.97%\n",
    "Epoch [2/10], Loss: 0.6943, Validation Accuracy: 52.11%\n",
    "Epoch [3/10], Loss: 1.3486, Validation Accuracy: 50.51%\n",
    "Epoch [4/10], Loss: 0.4996, Validation Accuracy: 51.67%\n",
    "Epoch [5/10], Loss: 0.3000, Validation Accuracy: 48.18%\n",
    "Epoch [6/10], Loss: 0.5406, Validation Accuracy: 49.49%\n",
    "Epoch [7/10], Loss: 0.4286, Validation Accuracy: 50.36%\n",
    "Epoch [8/10], Loss: 0.8129, Validation Accuracy: 50.95%\n",
    "Epoch [9/10], Loss: 0.7775, Validation Accuracy: 48.91%\n",
    "Epoch [10/10], Loss: 0.5658, Validation Accuracy: 49.49%\n",
    "Test Accuracy: 49.49%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c1fb3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    8248\n",
       "1    1342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd73904f",
   "metadata": {
    "id": "cd73904f",
    "outputId": "5e065b9f-0b14-4bb2-f07d-08931da9234d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5d073",
   "metadata": {
    "id": "dde5d073"
   },
   "source": [
    "1. Padding: different time stamps length, to account for that we do padding with mask so the model knows to take only relevant measurements\n",
    "2. Class imbalance: Given that the dataset has an imbalance of 86%, meaning one class represents 86% of the samples and the other class represents 14%, the class weights should be adjusted to compensate for the imbalance, such that the model gives more importance to the minority class (14%) and less importance to the majority class (86%). (try with values ([1.163, 7.143])\n",
    "3. DSSM combining LSTM for input and SSM for output\n",
    "4. Other hyperparameters to tune:\n",
    "- hidden_size\n",
    "- num_layers\n",
    "- num_static_features\n",
    "- transition_matrix\n",
    "- batch_size\n",
    "- learning_rate\n",
    "- num_epochs\n",
    "- max_grad_norm\n",
    "- optimizer\n",
    "- class_weights\n",
    "5. We are predicting mortality so actually different metrics would be better to use\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa2045",
   "metadata": {},
   "source": [
    "# Results step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c742f44",
   "metadata": {},
   "source": [
    "1. Rebalance to 60/40. `Epoch [10/10], Loss: 1.0779, Validation Accuracy: 47.74% Test Accuracy: 47.74%`\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0143ae04",
   "metadata": {
    "id": "0143ae04"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
